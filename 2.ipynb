{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import prep, trainer\n",
    "from models import emotion_recognition_cnn\n",
    "import keras as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 9475 negative examples\n",
      "loaded 3690 positive examples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 58, 58, 32)        1600      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 58, 58, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 25, 64)        51264     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 6, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 422,914\n",
      "Trainable params: 422,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "The trainer is NOT going to augment the data at runtime\n",
      "(11848, 64, 64, 1)\n",
      "(1317, 64, 64, 1)\n",
      "(11848, 2)\n",
      "(1317, 2)\n",
      "Epoch 1/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.7410- train accuracy:  0.8576130992572586\n",
      "- train loss:  0.5443810600835194\n",
      "- train roc_auc: 0.9086770631378251\n",
      "- train precision:  0.8224666142969363\n",
      "- train recall:  0.629017723039952\n",
      "- validation accuracy:  0.8496583143507973\n",
      "- validation loss:  0.3540061948058636\n",
      "- validation roc_auc: 0.8976836773722459\n",
      "- validation precision:  0.7963636363636364\n",
      "- validation recall:  0.6066481994459834\n",
      "186/186 [==============================] - 5s 26ms/step - loss: 0.5457 - acc: 0.7422 - val_loss: 0.3540 - val_acc: 0.8497\n",
      "Epoch 2/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8611- train accuracy:  0.8984638757596218\n",
      "- train loss:  0.33620348784023646\n",
      "- train roc_auc: 0.9442749162360418\n",
      "- train precision:  0.8152431791221827\n",
      "- train recall:  0.8257735055572244\n",
      "- validation accuracy:  0.8876233864844343\n",
      "- validation loss:  0.27705818545936\n",
      "- validation roc_auc: 0.9397477949443086\n",
      "- validation precision:  0.7933884297520661\n",
      "- validation recall:  0.7977839335180056\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.3355 - acc: 0.8614 - val_loss: 0.2771 - val_acc: 0.8876\n",
      "Epoch 3/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.2888 - acc: 0.8860- train accuracy:  0.9026839972991222\n",
      "- train loss:  0.28811375449671284\n",
      "- train roc_auc: 0.9488632939689774\n",
      "- train precision:  0.8217031342400947\n",
      "- train recall:  0.8347852207870231\n",
      "- validation accuracy:  0.8982536066818527\n",
      "- validation loss:  0.25691545521027226\n",
      "- validation roc_auc: 0.9474263725819725\n",
      "- validation precision:  0.8252148997134671\n",
      "- validation recall:  0.7977839335180056\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.2890 - acc: 0.8860 - val_loss: 0.2569 - val_acc: 0.8983\n",
      "Epoch 4/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.2693 - acc: 0.8944- train accuracy:  0.9167792032410533\n",
      "- train loss:  0.2688827521490294\n",
      "- train roc_auc: 0.9601172097738093\n",
      "- train precision:  0.8590254367146798\n",
      "- train recall:  0.8419945929708621\n",
      "- validation accuracy:  0.908883826879271\n",
      "- validation loss:  0.23291504364984\n",
      "- validation roc_auc: 0.9546355428319753\n",
      "- validation precision:  0.8513119533527697\n",
      "- validation recall:  0.8088642659279779\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.2681 - acc: 0.8947 - val_loss: 0.2329 - val_acc: 0.9089\n",
      "Epoch 5/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.9057- train accuracy:  0.9167792032410533\n",
      "- train loss:  0.24560896256419149\n",
      "- train roc_auc: 0.9625729700518175\n",
      "- train precision:  0.9126452976400141\n",
      "- train recall:  0.778311805346951\n",
      "- validation accuracy:  0.9050873196659074\n",
      "- validation loss:  0.23417351229134706\n",
      "- validation roc_auc: 0.9566812318177076\n",
      "- validation precision:  0.9097222222222222\n",
      "- validation recall:  0.7257617728531855\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.2466 - acc: 0.9057 - val_loss: 0.2342 - val_acc: 0.9051\n",
      "Epoch 6/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9061- train accuracy:  0.9148379473328832\n",
      "- train loss:  0.23796125646154878\n",
      "- train roc_auc: 0.9645774834200764\n",
      "- train precision:  0.830672748004561\n",
      "- train recall:  0.8753379393211175\n",
      "- validation accuracy:  0.908883826879271\n",
      "- validation loss:  0.24597409745464746\n",
      "- validation roc_auc: 0.9541951112089848\n",
      "- validation precision:  0.8301369863013699\n",
      "- validation recall:  0.8393351800554016\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.2369 - acc: 0.9057 - val_loss: 0.2460 - val_acc: 0.9089\n",
      "Epoch 7/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9130- train accuracy:  0.9322248480756246\n",
      "- train loss:  0.22356287479541498\n",
      "- train roc_auc: 0.9732615247573928\n",
      "- train precision:  0.8893341553637485\n",
      "- train recall:  0.8666266145989787\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.20550812754833797\n",
      "- validation roc_auc: 0.9636585959503472\n",
      "- validation precision:  0.8878787878787879\n",
      "- validation recall:  0.8116343490304709\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.2230 - acc: 0.9130 - val_loss: 0.2055 - val_acc: 0.9203\n",
      "Epoch 8/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9177- train accuracy:  0.9178764348413234\n",
      "- train loss:  0.21610795588287288\n",
      "- train roc_auc: 0.9729587364853803\n",
      "- train precision:  0.8144687666844634\n",
      "- train recall:  0.9164914388705316\n",
      "- validation accuracy:  0.9050873196659074\n",
      "- validation loss:  0.23241566229065713\n",
      "- validation roc_auc: 0.9638875044912435\n",
      "- validation precision:  0.7964824120603015\n",
      "- validation recall:  0.8781163434903048\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.2168 - acc: 0.9178 - val_loss: 0.2324 - val_acc: 0.9051\n",
      "Epoch 9/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.2134 - acc: 0.9166- train accuracy:  0.9309588116137745\n",
      "- train loss:  0.21258624279362218\n",
      "- train roc_auc: 0.9745380169240556\n",
      "- train precision:  0.8979397781299524\n",
      "- train recall:  0.8510063082006608\n",
      "- validation accuracy:  0.9286256643887624\n",
      "- validation loss:  0.20131529536135015\n",
      "- validation roc_auc: 0.964991481125187\n",
      "- validation precision:  0.9158878504672897\n",
      "- validation recall:  0.814404432132964\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.2129 - acc: 0.9172 - val_loss: 0.2013 - val_acc: 0.9286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.2062 - acc: 0.9194- train accuracy:  0.937711006076975\n",
      "- train loss:  0.20600955393530082\n",
      "- train roc_auc: 0.9785335386054693\n",
      "- train precision:  0.9135014363230131\n",
      "- train recall:  0.8597176329227997\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.19647743621375766\n",
      "- validation roc_auc: 0.9666199190996652\n",
      "- validation precision:  0.9034267912772586\n",
      "- validation recall:  0.8033240997229917\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.2058 - acc: 0.9196 - val_loss: 0.1965 - val_acc: 0.9226\n",
      "Epoch 11/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.2034 - acc: 0.9208- train accuracy:  0.9358541525995948\n",
      "- train loss:  0.2015508541096553\n",
      "- train roc_auc: 0.9785475196873202\n",
      "- train precision:  0.8781277597880482\n",
      "- train recall:  0.8960648843496546\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.20283077454693677\n",
      "- validation roc_auc: 0.9651885163249458\n",
      "- validation precision:  0.8647887323943662\n",
      "- validation recall:  0.850415512465374\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.2026 - acc: 0.9211 - val_loss: 0.2028 - val_acc: 0.9226\n",
      "Epoch 12/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.9223- train accuracy:  0.9403274814314653\n",
      "- train loss:  0.19502855144104067\n",
      "- train roc_auc: 0.9799708308440367\n",
      "- train precision:  0.9076492537313433\n",
      "- train recall:  0.8768398918594172\n",
      "- validation accuracy:  0.9248291571753986\n",
      "- validation loss:  0.1922950062253028\n",
      "- validation roc_auc: 0.9670922240637931\n",
      "- validation precision:  0.904320987654321\n",
      "- validation recall:  0.8116343490304709\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.1965 - acc: 0.9219 - val_loss: 0.1923 - val_acc: 0.9248\n",
      "Epoch 13/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9274- train accuracy:  0.9265698852126941\n",
      "- train loss:  0.18882013231536007\n",
      "- train roc_auc: 0.9775729166310381\n",
      "- train precision:  0.9518559353178978\n",
      "- train recall:  0.778011414839291\n",
      "- validation accuracy:  0.9096431283219438\n",
      "- validation loss:  0.23098051676989145\n",
      "- validation roc_auc: 0.9664750402763129\n",
      "- validation precision:  0.9514925373134329\n",
      "- validation recall:  0.7063711911357341\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.1887 - acc: 0.9273 - val_loss: 0.2310 - val_acc: 0.9096\n",
      "Epoch 14/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.9277- train accuracy:  0.9413403106009454\n",
      "- train loss:  0.18644028939533122\n",
      "- train roc_auc: 0.9821707355611127\n",
      "- train precision:  0.8846378504672897\n",
      "- train recall:  0.9098828477020127\n",
      "- validation accuracy:  0.9255884586180714\n",
      "- validation loss:  0.19159634758689317\n",
      "- validation roc_auc: 0.9682048934271374\n",
      "- validation precision:  0.8662952646239555\n",
      "- validation recall:  0.8614958448753463\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.1856 - acc: 0.9282 - val_loss: 0.1916 - val_acc: 0.9256\n",
      "Epoch 15/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9343- train accuracy:  0.9442943956785955\n",
      "- train loss:  0.1749959311659959\n",
      "- train roc_auc: 0.9839377468441102\n",
      "- train precision:  0.8989536621823617\n",
      "- train recall:  0.9032742565334936\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.19239000560844496\n",
      "- validation roc_auc: 0.967692022392471\n",
      "- validation precision:  0.8668555240793201\n",
      "- validation recall:  0.8476454293628809\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1745 - acc: 0.9337 - val_loss: 0.1924 - val_acc: 0.9226\n",
      "Epoch 16/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9350- train accuracy:  0.9513841998649561\n",
      "- train loss:  0.16893505744238627\n",
      "- train roc_auc: 0.986588087462404\n",
      "- train precision:  0.9284158107687519\n",
      "- train recall:  0.8960648843496546\n",
      "- validation accuracy:  0.9286256643887624\n",
      "- validation loss:  0.18120567345582994\n",
      "- validation roc_auc: 0.9712531438704667\n",
      "- validation precision:  0.9009009009009009\n",
      "- validation recall:  0.8310249307479224\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1685 - acc: 0.9352 - val_loss: 0.1812 - val_acc: 0.9286\n",
      "Epoch 17/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9350- train accuracy:  0.9472484807562458\n",
      "- train loss:  0.16516710342908533\n",
      "- train roc_auc: 0.9861346631710552\n",
      "- train precision:  0.8925667828106852\n",
      "- train recall:  0.9234004205467107\n",
      "- validation accuracy:  0.9164768413059985\n",
      "- validation loss:  0.204915923948259\n",
      "- validation roc_auc: 0.9659360910534429\n",
      "- validation precision:  0.8364611260053619\n",
      "- validation recall:  0.8642659279778393\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1654 - acc: 0.9349 - val_loss: 0.2049 - val_acc: 0.9165\n",
      "Epoch 18/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.1630 - acc: 0.9358- train accuracy:  0.9572923700202566\n",
      "- train loss:  0.16284105677728633\n",
      "- train roc_auc: 0.9891393439949454\n",
      "- train precision:  0.9291577987230161\n",
      "- train recall:  0.9179933914088315\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.18382398129052616\n",
      "- validation roc_auc: 0.9695174955667081\n",
      "- validation precision:  0.8664772727272727\n",
      "- validation recall:  0.8448753462603878\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1625 - acc: 0.9360 - val_loss: 0.1838 - val_acc: 0.9218\n",
      "Epoch 19/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9380- train accuracy:  0.9513841998649561\n",
      "- train loss:  0.15960419685111346\n",
      "- train roc_auc: 0.9867806931732228\n",
      "- train precision:  0.9335433070866141\n",
      "- train recall:  0.8903574647041154\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.20754513524659401\n",
      "- validation roc_auc: 0.9616071118116807\n",
      "- validation precision:  0.8926380368098159\n",
      "- validation recall:  0.8060941828254847\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1612 - acc: 0.9376 - val_loss: 0.2075 - val_acc: 0.9203\n",
      "Epoch 20/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9405- train accuracy:  0.9579675894665767\n",
      "- train loss:  0.15598475294166284\n",
      "- train roc_auc: 0.9905228981030193\n",
      "- train precision:  0.9259705085765875\n",
      "- train recall:  0.9243015920696906\n",
      "- validation accuracy:  0.9240698557327259\n",
      "- validation loss:  0.19290712049221034\n",
      "- validation roc_auc: 0.9679962679215104\n",
      "- validation precision:  0.8760806916426513\n",
      "- validation recall:  0.8421052631578947\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1564 - acc: 0.9405 - val_loss: 0.1929 - val_acc: 0.9241\n",
      "Epoch 21/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9414- train accuracy:  0.9600776502363269\n",
      "- train loss:  0.14974851949815812\n",
      "- train roc_auc: 0.9912308292128518\n",
      "- train precision:  0.9257602862254025\n",
      "- train recall:  0.9327125262841695\n",
      "- validation accuracy:  0.9240698557327259\n",
      "- validation loss:  0.1898169164618671\n",
      "- validation roc_auc: 0.9684193140856987\n",
      "- validation precision:  0.8739255014326648\n",
      "- validation recall:  0.8448753462603878\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1493 - acc: 0.9409 - val_loss: 0.1898 - val_acc: 0.9241\n",
      "Epoch 22/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9434- train accuracy:  0.9621033085752869\n",
      "- train loss:  0.14026266300219448\n",
      "- train roc_auc: 0.9929976289284064\n",
      "- train precision:  0.9183614177803603\n",
      "- train recall:  0.9495343947131271\n",
      "- validation accuracy:  0.9240698557327259\n",
      "- validation loss:  0.19553943389097486\n",
      "- validation roc_auc: 0.9679180333569002\n",
      "- validation precision:  0.859504132231405\n",
      "- validation recall:  0.8642659279778393\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1406 - acc: 0.9435 - val_loss: 0.1955 - val_acc: 0.9241\n",
      "Epoch 23/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.9452- train accuracy:  0.9680114787305875\n",
      "- train loss:  0.14100395060401563\n",
      "- train roc_auc: 0.9940918557430212\n",
      "- train precision:  0.9638364779874213\n",
      "- train recall:  0.9206969059777711\n",
      "- validation accuracy:  0.9233105542900532\n",
      "- validation loss:  0.1943597458331261\n",
      "- validation roc_auc: 0.968642427473661\n",
      "- validation precision:  0.90625\n",
      "- validation recall:  0.8033240997229917\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1405 - acc: 0.9451 - val_loss: 0.1944 - val_acc: 0.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.1309 - acc: 0.9506- train accuracy:  0.9734132343011479\n",
      "- train loss:  0.13164388939454036\n",
      "- train roc_auc: 0.9952119731234593\n",
      "- train precision:  0.9605745721271394\n",
      "- train recall:  0.9441273655752478\n",
      "- validation accuracy:  0.9263477600607442\n",
      "- validation loss:  0.18998057971692212\n",
      "- validation roc_auc: 0.9687235596147383\n",
      "- validation precision:  0.9\n",
      "- validation recall:  0.8227146814404432\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.1310 - acc: 0.9504 - val_loss: 0.1900 - val_acc: 0.9263\n",
      "Epoch 25/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9537- train accuracy:  0.9691087103308575\n",
      "- train loss:  0.122702665259756\n",
      "- train roc_auc: 0.9948079762759553\n",
      "- train precision:  0.9340755933196602\n",
      "- train recall:  0.9576449384199459\n",
      "- validation accuracy:  0.9248291571753986\n",
      "- validation loss:  0.18995569905459925\n",
      "- validation roc_auc: 0.9709488983414272\n",
      "- validation precision:  0.8540540540540541\n",
      "- validation recall:  0.8753462603878116\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1235 - acc: 0.9538 - val_loss: 0.1900 - val_acc: 0.9248\n",
      "Epoch 26/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9546- train accuracy:  0.9772113436866982\n",
      "- train loss:  0.11719894072872251\n",
      "- train roc_auc: 0.9964307690853844\n",
      "- train precision:  0.9661688509600731\n",
      "- train recall:  0.9522379092820666\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.20052822897084843\n",
      "- validation roc_auc: 0.9706620382711899\n",
      "- validation precision:  0.884272997032641\n",
      "- validation recall:  0.8254847645429363\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.1167 - acc: 0.9547 - val_loss: 0.2005 - val_acc: 0.9226\n",
      "Epoch 27/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9552- train accuracy:  0.9759453072248481\n",
      "- train loss:  0.11924796696352363\n",
      "- train roc_auc: 0.9965838381303136\n",
      "- train precision:  0.9703337453646477\n",
      "- train recall:  0.9432261940522679\n",
      "- validation accuracy:  0.9233105542900532\n",
      "- validation loss:  0.2036995033287332\n",
      "- validation roc_auc: 0.9675087506809305\n",
      "- validation precision:  0.8987730061349694\n",
      "- validation recall:  0.8116343490304709\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.1192 - acc: 0.9551 - val_loss: 0.2037 - val_acc: 0.9233\n",
      "Epoch 28/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9574- train accuracy:  0.9716407832545577\n",
      "- train loss:  0.11122516230513405\n",
      "- train roc_auc: 0.9956335476993433\n",
      "- train precision:  0.9514328808446455\n",
      "- train recall:  0.9474316611595074\n",
      "- validation accuracy:  0.9210326499620349\n",
      "- validation loss:  0.21487437936512013\n",
      "- validation roc_auc: 0.9642236233614204\n",
      "- validation precision:  0.8790560471976401\n",
      "- validation recall:  0.8254847645429363\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1109 - acc: 0.9573 - val_loss: 0.2149 - val_acc: 0.9210\n",
      "Epoch 29/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.1109 - acc: 0.9578- train accuracy:  0.9665766374071574\n",
      "- train loss:  0.11235570028590279\n",
      "- train roc_auc: 0.9946564939868477\n",
      "- train precision:  0.9710889816896884\n",
      "- train recall:  0.9080805046560528\n",
      "- validation accuracy:  0.908883826879271\n",
      "- validation loss:  0.22184391861715005\n",
      "- validation roc_auc: 0.9627255763279593\n",
      "- validation precision:  0.889967637540453\n",
      "- validation recall:  0.7617728531855956\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1119 - acc: 0.9575 - val_loss: 0.2218 - val_acc: 0.9089\n",
      "Epoch 30/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9599- train accuracy:  0.9853983794733289\n",
      "- train loss:  0.1059373540540932\n",
      "- train roc_auc: 0.998680542011811\n",
      "- train precision:  0.9879406307977736\n",
      "- train recall:  0.9597476719735656\n",
      "- validation accuracy:  0.9309035687167806\n",
      "- validation loss:  0.20560246116524858\n",
      "- validation roc_auc: 0.9697753798722748\n",
      "- validation precision:  0.9166666666666666\n",
      "- validation recall:  0.8227146814404432\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.1055 - acc: 0.9600 - val_loss: 0.2056 - val_acc: 0.9309\n",
      "Epoch 31/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9646- train accuracy:  0.9848075624577988\n",
      "- train loss:  0.09581919313844069\n",
      "- train roc_auc: 0.998680612534292\n",
      "- train precision:  0.9723972397239724\n",
      "- train recall:  0.9735656353259237\n",
      "- validation accuracy:  0.9316628701594533\n",
      "- validation loss:  0.20461034933182898\n",
      "- validation roc_auc: 0.9716240336582482\n",
      "- validation precision:  0.8795518207282913\n",
      "- validation recall:  0.8698060941828255\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0954 - acc: 0.9647 - val_loss: 0.2046 - val_acc: 0.9317\n",
      "Epoch 32/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9646- train accuracy:  0.9851451721809588\n",
      "- train loss:  0.09336537896334159\n",
      "- train roc_auc: 0.9987420376152103\n",
      "- train precision:  0.9876275904732447\n",
      "- train recall:  0.9591468909582457\n",
      "- validation accuracy:  0.9263477600607442\n",
      "- validation loss:  0.22798851546502602\n",
      "- validation roc_auc: 0.964379368096524\n",
      "- validation precision:  0.9074074074074074\n",
      "- validation recall:  0.814404432132964\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.0930 - acc: 0.9646 - val_loss: 0.2280 - val_acc: 0.9263\n",
      "Epoch 33/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9620- train accuracy:  0.9872552329507089\n",
      "- train loss:  0.09705416213753737\n",
      "- train roc_auc: 0.9991039589875101\n",
      "- train precision:  0.9892241379310345\n",
      "- train recall:  0.9651547011114449\n",
      "- validation accuracy:  0.9233105542900532\n",
      "- validation loss:  0.23457880031908654\n",
      "- validation roc_auc: 0.9664866305821811\n",
      "- validation precision:  0.891566265060241\n",
      "- validation recall:  0.8199445983379502\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.0978 - acc: 0.9620 - val_loss: 0.2346 - val_acc: 0.9233\n",
      "Epoch 34/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9648- train accuracy:  0.9898717083051992\n",
      "- train loss:  0.0916089129525411\n",
      "- train roc_auc: 0.9993191230769268\n",
      "- train precision:  0.9884322678843227\n",
      "- train recall:  0.9753679783718835\n",
      "- validation accuracy:  0.9164768413059985\n",
      "- validation loss:  0.2166031794445752\n",
      "- validation roc_auc: 0.9651566429838083\n",
      "- validation precision:  0.8680351906158358\n",
      "- validation recall:  0.8199445983379502\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0912 - acc: 0.9649 - val_loss: 0.2166 - val_acc: 0.9165\n",
      "Epoch 35/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9707- train accuracy:  0.987846049966239\n",
      "- train loss:  0.08128601521791878\n",
      "- train roc_auc: 0.9991043821223959\n",
      "- train precision:  0.9749478079331941\n",
      "- train recall:  0.9819765695404026\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.246140261377995\n",
      "- validation roc_auc: 0.9625372338576015\n",
      "- validation precision:  0.8627450980392157\n",
      "- validation recall:  0.853185595567867\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0810 - acc: 0.9707 - val_loss: 0.2461 - val_acc: 0.9226\n",
      "Epoch 36/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0771 - acc: 0.9711- train accuracy:  0.9912221471978393\n",
      "- train loss:  0.07787351187643678\n",
      "- train roc_auc: 0.9994621074070784\n",
      "- train precision:  0.9840888622035425\n",
      "- train recall:  0.9846800841093422\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.22360851650176486\n",
      "- validation roc_auc: 0.9652493654307538\n",
      "- validation precision:  0.867816091954023\n",
      "- validation recall:  0.8365650969529086\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.0775 - acc: 0.9710 - val_loss: 0.2236 - val_acc: 0.9203\n",
      "Epoch 37/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9688- train accuracy:  0.9801654287643484\n",
      "- train loss:  0.08053405712195782\n",
      "- train roc_auc: 0.9992064281523487\n",
      "- train precision:  0.937995469988675\n",
      "- train recall:  0.9951937518774406\n",
      "- validation accuracy:  0.9111617312072893\n",
      "- validation loss:  0.26078184685565886\n",
      "- validation roc_auc: 0.9625488241634697\n",
      "- validation precision:  0.8128205128205128\n",
      "- validation recall:  0.8781163434903048\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0805 - acc: 0.9690 - val_loss: 0.2608 - val_acc: 0.9112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0756 - acc: 0.9719- train accuracy:  0.987846049966239\n",
      "- train loss:  0.07578662232226069\n",
      "- train roc_auc: 0.9992340729648861\n",
      "- train precision:  0.9844843322178278\n",
      "- train recall:  0.9720636827876239\n",
      "- validation accuracy:  0.9126803340926348\n",
      "- validation loss:  0.2716094065364238\n",
      "- validation roc_auc: 0.9599967836901215\n",
      "- validation precision:  0.8819875776397516\n",
      "- validation recall:  0.7867036011080333\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0755 - acc: 0.9719 - val_loss: 0.2716 - val_acc: 0.9127\n",
      "Epoch 39/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9718- train accuracy:  0.9957798784604996\n",
      "- train loss:  0.07234967402274095\n",
      "- train roc_auc: 0.9998729361199257\n",
      "- train precision:  0.9901345291479821\n",
      "- train recall:  0.9948933613697807\n",
      "- validation accuracy:  0.9278663629460896\n",
      "- validation loss:  0.25128463838438964\n",
      "- validation roc_auc: 0.9662548244648176\n",
      "- validation precision:  0.88\n",
      "- validation recall:  0.853185595567867\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0720 - acc: 0.9716 - val_loss: 0.2513 - val_acc: 0.9279\n",
      "Epoch 40/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9768- train accuracy:  0.9940918298446996\n",
      "- train loss:  0.0653827392321978\n",
      "- train roc_auc: 0.9997223001005897\n",
      "- train precision:  0.9900751879699248\n",
      "- train recall:  0.9888855512165815\n",
      "- validation accuracy:  0.9210326499620349\n",
      "- validation loss:  0.26472506628583214\n",
      "- validation roc_auc: 0.9640113758852096\n",
      "- validation precision:  0.8619718309859155\n",
      "- validation recall:  0.8476454293628809\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0651 - acc: 0.9766 - val_loss: 0.2647 - val_acc: 0.9210\n",
      "Epoch 41/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9757- train accuracy:  0.9968771100607697\n",
      "- train loss:  0.06332390979913383\n",
      "- train roc_auc: 0.9999156374821485\n",
      "- train precision:  0.9925194494314782\n",
      "- train recall:  0.9963953139080806\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.2835717455129689\n",
      "- validation roc_auc: 0.9613637153884491\n",
      "- validation precision:  0.8539325842696629\n",
      "- validation recall:  0.8421052631578947\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0631 - acc: 0.9756 - val_loss: 0.2836 - val_acc: 0.9172\n",
      "Epoch 42/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9781- train accuracy:  0.9932478055367995\n",
      "- train loss:  0.059927302163036426\n",
      "- train roc_auc: 0.9997724768457945\n",
      "- train precision:  0.9894546550165713\n",
      "- train recall:  0.9864824271553019\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.25669407119285725\n",
      "- validation roc_auc: 0.9637621843090439\n",
      "- validation precision:  0.8839285714285714\n",
      "- validation recall:  0.8227146814404432\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0597 - acc: 0.9781 - val_loss: 0.2567 - val_acc: 0.9218\n",
      "Epoch 43/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9788- train accuracy:  0.9965395003376097\n",
      "- train loss:  0.061017103805279034\n",
      "- train roc_auc: 0.9998800236292624\n",
      "- train precision:  0.9919210053859964\n",
      "- train recall:  0.9957945328927605\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.2758903018705937\n",
      "- validation roc_auc: 0.9636202030621588\n",
      "- validation precision:  0.8611898016997167\n",
      "- validation recall:  0.8421052631578947\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0607 - acc: 0.9787 - val_loss: 0.2759 - val_acc: 0.9195\n",
      "Epoch 44/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9718- train accuracy:  0.9946826468602296\n",
      "- train loss:  0.07289528181168937\n",
      "- train roc_auc: 0.9998237819506948\n",
      "- train precision:  0.9936517533252721\n",
      "- train recall:  0.9873835986782817\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.26590256137248686\n",
      "- validation roc_auc: 0.9639099607088631\n",
      "- validation precision:  0.9034267912772586\n",
      "- validation recall:  0.8033240997229917\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0731 - acc: 0.9716 - val_loss: 0.2659 - val_acc: 0.9226\n",
      "Epoch 45/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9781- train accuracy:  0.9980587440918298\n",
      "- train loss:  0.05784569669623056\n",
      "- train roc_auc: 0.9999078800092427\n",
      "- train precision:  0.9978915662650603\n",
      "- train recall:  0.9951937518774406\n",
      "- validation accuracy:  0.9248291571753986\n",
      "- validation loss:  0.27407317860341923\n",
      "- validation roc_auc: 0.9672820153223843\n",
      "- validation precision:  0.8852941176470588\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0579 - acc: 0.9783 - val_loss: 0.2741 - val_acc: 0.9248\n",
      "Epoch 46/500\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9812- train accuracy:  0.9977211343686698\n",
      "- train loss:  0.05242825394080025\n",
      "- train roc_auc: 0.9999639101203674\n",
      "- train precision:  0.9940155595451825\n",
      "- train recall:  0.9978972664463803\n",
      "- validation accuracy:  0.9240698557327259\n",
      "- validation loss:  0.30665187321563697\n",
      "- validation roc_auc: 0.9625944609928256\n",
      "- validation precision:  0.8635097493036211\n",
      "- validation recall:  0.8587257617728532\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.0523 - acc: 0.9811 - val_loss: 0.3067 - val_acc: 0.9241\n",
      "Epoch 47/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9790- train accuracy:  0.9925725860904794\n",
      "- train loss:  0.05625369544483716\n",
      "- train roc_auc: 0.9997374271727562\n",
      "- train precision:  0.9815750371471025\n",
      "- train recall:  0.9921898468008411\n",
      "- validation accuracy:  0.9141989369779803\n",
      "- validation loss:  0.3261946159144107\n",
      "- validation roc_auc: 0.9580995954983252\n",
      "- validation precision:  0.8463687150837989\n",
      "- validation recall:  0.8393351800554016\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.0561 - acc: 0.9790 - val_loss: 0.3262 - val_acc: 0.9142\n",
      "Epoch 48/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9779- train accuracy:  0.9956110735989196\n",
      "- train loss:  0.057652032620334526\n",
      "- train roc_auc: 0.9998653549532224\n",
      "- train precision:  0.995164702326987\n",
      "- train recall:  0.9891859417242416\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.31180710848435\n",
      "- validation roc_auc: 0.9554099201427926\n",
      "- validation precision:  0.8854489164086687\n",
      "- validation recall:  0.7922437673130194\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0580 - acc: 0.9774 - val_loss: 0.3118 - val_acc: 0.9150\n",
      "Epoch 49/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9798- train accuracy:  0.9980587440918298\n",
      "- train loss:  0.05503483471170742\n",
      "- train roc_auc: 0.9999457505815195\n",
      "- train precision:  0.9972924187725631\n",
      "- train recall:  0.9957945328927605\n",
      "- validation accuracy:  0.9096431283219438\n",
      "- validation loss:  0.32191266957882597\n",
      "- validation roc_auc: 0.9577410204105287\n",
      "- validation precision:  0.8689024390243902\n",
      "- validation recall:  0.7894736842105263\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0549 - acc: 0.9800 - val_loss: 0.3219 - val_acc: 0.9096\n",
      "Epoch 50/500\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9797- train accuracy:  0.9978899392302498\n",
      "- train loss:  0.053896040167569284\n",
      "- train roc_auc: 0.9999565052598663\n",
      "- train precision:  0.9957983193277311\n",
      "- train recall:  0.9966957044157405\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.29070355044501067\n",
      "- validation roc_auc: 0.9590166784501444\n",
      "- validation precision:  0.8888888888888888\n",
      "- validation recall:  0.8199445983379502\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0536 - acc: 0.9796 - val_loss: 0.2907 - val_acc: 0.9226\n",
      "Epoch 51/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9812- train accuracy:  0.9973835246455098\n",
      "- train loss:  0.05037575875359269\n",
      "- train roc_auc: 0.9999398266931188\n",
      "- train precision:  0.993122009569378\n",
      "- train recall:  0.9975968759387204\n",
      "- validation accuracy:  0.9271070615034168\n",
      "- validation loss:  0.2741607113959849\n",
      "- validation roc_auc: 0.9681027538566742\n",
      "- validation precision:  0.8796561604584527\n",
      "- validation recall:  0.850415512465374\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0502 - acc: 0.9814 - val_loss: 0.2742 - val_acc: 0.9271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9787- train accuracy:  0.9980587440918298\n",
      "- train loss:  0.05437243422889512\n",
      "- train roc_auc: 0.9999584093668523\n",
      "- train precision:  0.9955035971223022\n",
      "- train recall:  0.9975968759387204\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.3204913016241793\n",
      "- validation roc_auc: 0.9614781696588972\n",
      "- validation precision:  0.8607954545454546\n",
      "- validation recall:  0.8393351800554016\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0545 - acc: 0.9786 - val_loss: 0.3205 - val_acc: 0.9188\n",
      "Epoch 53/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9810- train accuracy:  0.9945982444294396\n",
      "- train loss:  0.05225595168563098\n",
      "- train roc_auc: 0.9998700446982063\n",
      "- train precision:  0.9966534834195315\n",
      "- train recall:  0.9840793030940223\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.32958653345194927\n",
      "- validation roc_auc: 0.9619265696171722\n",
      "- validation precision:  0.9047619047619048\n",
      "- validation recall:  0.7894736842105263\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0523 - acc: 0.9811 - val_loss: 0.3296 - val_acc: 0.9195\n",
      "Epoch 54/500\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9829- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.04840191446054805\n",
      "- train roc_auc: 0.9999756873746881\n",
      "- train precision:  0.9973021582733813\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.31147243217165577\n",
      "- validation roc_auc: 0.9623097741049387\n",
      "- validation precision:  0.8587896253602305\n",
      "- validation recall:  0.8254847645429363\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0482 - acc: 0.9827 - val_loss: 0.3115 - val_acc: 0.9150\n",
      "Epoch 55/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9813- train accuracy:  0.9963706954760297\n",
      "- train loss:  0.05222803623805356\n",
      "- train roc_auc: 0.9999288251860885\n",
      "- train precision:  0.9886972040452112\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.3008676265665502\n",
      "- validation roc_auc: 0.9616027654469801\n",
      "- validation precision:  0.8480662983425414\n",
      "- validation recall:  0.850415512465374\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0520 - acc: 0.9810 - val_loss: 0.3009 - val_acc: 0.9172\n",
      "Epoch 56/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9836- train accuracy:  0.9974679270762998\n",
      "- train loss:  0.04606876447910014\n",
      "- train roc_auc: 0.9999748058436762\n",
      "- train precision:  0.9975867269984917\n",
      "- train recall:  0.9933914088314809\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.30884723518052243\n",
      "- validation roc_auc: 0.9612427415709501\n",
      "- validation precision:  0.8923076923076924\n",
      "- validation recall:  0.8033240997229917\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0460 - acc: 0.9836 - val_loss: 0.3088 - val_acc: 0.9195\n",
      "Epoch 57/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9836- train accuracy:  0.9978055367994598\n",
      "- train loss:  0.04416028250311926\n",
      "- train roc_auc: 0.999958162538169\n",
      "- train precision:  0.9978896593307205\n",
      "- train recall:  0.9942925803544608\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.33529188667813664\n",
      "- validation roc_auc: 0.9613687861472664\n",
      "- validation precision:  0.9102564102564102\n",
      "- validation recall:  0.7867036011080333\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0440 - acc: 0.9837 - val_loss: 0.3353 - val_acc: 0.9203\n",
      "Epoch 58/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9839- train accuracy:  0.9980587440918298\n",
      "- train loss:  0.04275977638020142\n",
      "- train roc_auc: 0.999960207690117\n",
      "- train precision:  0.9946140035906643\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9096431283219438\n",
      "- validation loss:  0.33331935439222043\n",
      "- validation roc_auc: 0.9580648245807207\n",
      "- validation precision:  0.8517441860465116\n",
      "- validation recall:  0.8116343490304709\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0426 - acc: 0.9840 - val_loss: 0.3333 - val_acc: 0.9096\n",
      "Epoch 59/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9812- train accuracy:  0.9982275489534098\n",
      "- train loss:  0.04670444482674597\n",
      "- train roc_auc: 0.9999885224662233\n",
      "- train precision:  0.9955062911923307\n",
      "- train recall:  0.9981976569540403\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.30662967150376075\n",
      "- validation roc_auc: 0.9638809849441927\n",
      "- validation precision:  0.8623595505617978\n",
      "- validation recall:  0.850415512465374\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0473 - acc: 0.9813 - val_loss: 0.3066 - val_acc: 0.9226\n",
      "Epoch 60/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9834- train accuracy:  0.9979743416610398\n",
      "- train loss:  0.048138621530466025\n",
      "- train roc_auc: 0.9999361947853491\n",
      "- train precision:  0.9969924812030075\n",
      "- train recall:  0.9957945328927605\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.3113000871742324\n",
      "- validation roc_auc: 0.9616310168175338\n",
      "- validation precision:  0.8957055214723927\n",
      "- validation recall:  0.8088642659279779\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0481 - acc: 0.9833 - val_loss: 0.3113 - val_acc: 0.9218\n",
      "Epoch 61/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9814- train accuracy:  0.9950202565833896\n",
      "- train loss:  0.05117902387081973\n",
      "- train roc_auc: 0.999789578547428\n",
      "- train precision:  0.9901079136690647\n",
      "- train recall:  0.9921898468008411\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.3061483698807255\n",
      "- validation roc_auc: 0.9618548545996128\n",
      "- validation precision:  0.8764705882352941\n",
      "- validation recall:  0.8254847645429363\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0524 - acc: 0.9813 - val_loss: 0.3061 - val_acc: 0.9203\n",
      "Epoch 62/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9824- train accuracy:  0.9989027683997299\n",
      "- train loss:  0.054229756913887234\n",
      "- train roc_auc: 0.9999286136186456\n",
      "- train precision:  0.9964071856287425\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.29611805205826697\n",
      "- validation roc_auc: 0.9602865413368258\n",
      "- validation precision:  0.86\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.0540 - acc: 0.9819 - val_loss: 0.2961 - val_acc: 0.9172\n",
      "Epoch 63/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9857- train accuracy:  0.9992403781228899\n",
      "- train loss:  0.040402461465883305\n",
      "- train roc_auc: 0.9999643685164936\n",
      "- train precision:  0.9987980769230769\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9271070615034168\n",
      "- validation loss:  0.3377526620616493\n",
      "- validation roc_auc: 0.963813616291334\n",
      "- validation precision:  0.8690807799442897\n",
      "- validation recall:  0.8642659279778393\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0403 - acc: 0.9858 - val_loss: 0.3378 - val_acc: 0.9271\n",
      "Epoch 64/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9869- train accuracy:  0.9977211343686698\n",
      "- train loss:  0.03688429764413465\n",
      "- train roc_auc: 0.9999723728180829\n",
      "- train precision:  0.9978890229191797\n",
      "- train recall:  0.9939921898468008\n",
      "- validation accuracy:  0.9210326499620349\n",
      "- validation loss:  0.37691091337073634\n",
      "- validation roc_auc: 0.9596758770963967\n",
      "- validation precision:  0.9053627760252366\n",
      "- validation recall:  0.7950138504155124\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.0367 - acc: 0.9869 - val_loss: 0.3769 - val_acc: 0.9210\n",
      "Epoch 65/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9867- train accuracy:  0.9981431465226198\n",
      "- train loss:  0.034798588833893015\n",
      "- train roc_auc: 0.9999829511902273\n",
      "- train precision:  0.9946156147173197\n",
      "- train recall:  0.9987984379693602\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.3753881575758926\n",
      "- validation roc_auc: 0.9614651305647957\n",
      "- validation precision:  0.8429752066115702\n",
      "- validation recall:  0.8476454293628809\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0349 - acc: 0.9867 - val_loss: 0.3754 - val_acc: 0.9150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9843- train accuracy:  0.9944294395678596\n",
      "- train loss:  0.042175769333329595\n",
      "- train roc_auc: 0.9999689877389968\n",
      "- train precision:  0.9805596465390279\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9096431283219438\n",
      "- validation loss:  0.3393064937812411\n",
      "- validation roc_auc: 0.9616266704528331\n",
      "- validation precision:  0.8040201005025126\n",
      "- validation recall:  0.8864265927977839\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0420 - acc: 0.9843 - val_loss: 0.3393 - val_acc: 0.9096\n",
      "Epoch 67/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9863- train accuracy:  0.9981431465226198\n",
      "- train loss:  0.03865480177974798\n",
      "- train roc_auc: 0.9999762868157763\n",
      "- train precision:  0.9996977938954367\n",
      "- train recall:  0.9936917993391409\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.3556563137332508\n",
      "- validation roc_auc: 0.9596729795199297\n",
      "- validation precision:  0.9064516129032258\n",
      "- validation recall:  0.778393351800554\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0391 - acc: 0.9859 - val_loss: 0.3557 - val_acc: 0.9172\n",
      "Epoch 68/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9830- train accuracy:  0.9988183659689399\n",
      "- train loss:  0.04542477254046456\n",
      "- train roc_auc: 0.9999843263786061\n",
      "- train precision:  0.9981965734896303\n",
      "- train recall:  0.9975968759387204\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.33053428251451855\n",
      "- validation roc_auc: 0.9635876053269046\n",
      "- validation precision:  0.8807339449541285\n",
      "- validation recall:  0.7977839335180056\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0452 - acc: 0.9831 - val_loss: 0.3305 - val_acc: 0.9150\n",
      "Epoch 69/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9893- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.02911733164759208\n",
      "- train roc_auc: 0.9999924364639168\n",
      "- train precision:  0.9970041941282205\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.3601736493335136\n",
      "- validation roc_auc: 0.9617259124468296\n",
      "- validation precision:  0.8559322033898306\n",
      "- validation recall:  0.8393351800554016\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0290 - acc: 0.9893 - val_loss: 0.3602 - val_acc: 0.9172\n",
      "Epoch 70/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9872- train accuracy:  0.9989871708305199\n",
      "- train loss:  0.0345323249894087\n",
      "- train roc_auc: 0.9999922601577144\n",
      "- train precision:  0.9975997599759976\n",
      "- train recall:  0.9987984379693602\n",
      "- validation accuracy:  0.9210326499620349\n",
      "- validation loss:  0.35364397502615313\n",
      "- validation roc_auc: 0.96119130958866\n",
      "- validation precision:  0.8599439775910365\n",
      "- validation recall:  0.850415512465374\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0344 - acc: 0.9871 - val_loss: 0.3536 - val_acc: 0.9210\n",
      "Epoch 71/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9851- train accuracy:  0.99949358541526\n",
      "- train loss:  0.03852220386289347\n",
      "- train roc_auc: 0.9999894039972353\n",
      "- train precision:  0.9987991594115881\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.932422171602126\n",
      "- validation loss:  0.33397405457116475\n",
      "- validation roc_auc: 0.9664076716234541\n",
      "- validation precision:  0.8908045977011494\n",
      "- validation recall:  0.8587257617728532\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0384 - acc: 0.9850 - val_loss: 0.3340 - val_acc: 0.9324\n",
      "Epoch 72/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9849- train accuracy:  0.9945138419986496\n",
      "- train loss:  0.04375883004553289\n",
      "- train roc_auc: 0.9998485000802722\n",
      "- train precision:  0.9954462659380692\n",
      "- train recall:  0.9849804746170021\n",
      "- validation accuracy:  0.9263477600607442\n",
      "- validation loss:  0.36677705743770844\n",
      "- validation roc_auc: 0.9630457585275676\n",
      "- validation precision:  0.9230769230769231\n",
      "- validation recall:  0.7977839335180056\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0437 - acc: 0.9849 - val_loss: 0.3668 - val_acc: 0.9263\n",
      "Epoch 73/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9885- train accuracy:  0.9979743416610398\n",
      "- train loss:  0.03736048185544505\n",
      "- train roc_auc: 0.9999789666700529\n",
      "- train precision:  0.9984917043740573\n",
      "- train recall:  0.9942925803544608\n",
      "- validation accuracy:  0.9248291571753986\n",
      "- validation loss:  0.3300278029300629\n",
      "- validation roc_auc: 0.9656180820361848\n",
      "- validation precision:  0.904320987654321\n",
      "- validation recall:  0.8116343490304709\n",
      "186/186 [==============================] - 3s 18ms/step - loss: 0.0372 - acc: 0.9886 - val_loss: 0.3300 - val_acc: 0.9248\n",
      "Epoch 74/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9825- train accuracy:  0.9972991222147198\n",
      "- train loss:  0.049023473341644294\n",
      "- train roc_auc: 0.9999537196218684\n",
      "- train precision:  0.9919427036705462\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9141989369779803\n",
      "- validation loss:  0.327895540152885\n",
      "- validation roc_auc: 0.9605697794364794\n",
      "- validation precision:  0.8522727272727273\n",
      "- validation recall:  0.8310249307479224\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0494 - acc: 0.9824 - val_loss: 0.3279 - val_acc: 0.9142\n",
      "Epoch 75/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9839- train accuracy:  0.9984807562457799\n",
      "- train loss:  0.044073473458126226\n",
      "- train roc_auc: 0.9999903913119688\n",
      "- train precision:  0.9955103262496259\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.33121831609882363\n",
      "- validation roc_auc: 0.9608392540479144\n",
      "- validation precision:  0.8410958904109589\n",
      "- validation recall:  0.850415512465374\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0439 - acc: 0.9839 - val_loss: 0.3312 - val_acc: 0.9150\n",
      "Epoch 76/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9880- train accuracy:  0.9994091829844699\n",
      "- train loss:  0.035776898131647965\n",
      "- train roc_auc: 0.9999963857228507\n",
      "- train precision:  0.9990985576923077\n",
      "- train recall:  0.9987984379693602\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.35107430289573716\n",
      "- validation roc_auc: 0.9636564227679969\n",
      "- validation precision:  0.8706896551724138\n",
      "- validation recall:  0.8393351800554016\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0356 - acc: 0.9878 - val_loss: 0.3511 - val_acc: 0.9218\n",
      "Epoch 77/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9915- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.02726877171729844\n",
      "- train roc_auc: 0.9999971262089008\n",
      "- train precision:  0.9970041941282205\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.37190639807495457\n",
      "- validation roc_auc: 0.9606682970363587\n",
      "- validation precision:  0.8695652173913043\n",
      "- validation recall:  0.8310249307479224\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0273 - acc: 0.9915 - val_loss: 0.3719 - val_acc: 0.9195\n",
      "Epoch 78/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9872- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.03417849661327256\n",
      "- train roc_auc: 0.9999950810569529\n",
      "- train precision:  0.9979010494752624\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.3785096473918327\n",
      "- validation roc_auc: 0.96378319173843\n",
      "- validation precision:  0.842391304347826\n",
      "- validation recall:  0.8587257617728532\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0340 - acc: 0.9871 - val_loss: 0.3785 - val_acc: 0.9172\n",
      "Epoch 79/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9840- train accuracy:  0.9989027683997299\n",
      "- train loss:  0.0416318014991054\n",
      "- train roc_auc: 0.9999945168771052\n",
      "- train precision:  0.9964071856287425\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.36376806023271135\n",
      "- validation roc_auc: 0.9625430290105357\n",
      "- validation precision:  0.8448753462603878\n",
      "- validation recall:  0.8448753462603878\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0417 - acc: 0.9842 - val_loss: 0.3638 - val_acc: 0.9150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9903- train accuracy:  0.99949358541526\n",
      "- train loss:  0.03223187428132545\n",
      "- train roc_auc: 0.9999976551275079\n",
      "- train precision:  0.9990988284770201\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.371635255844374\n",
      "- validation roc_auc: 0.9627777327043661\n",
      "- validation precision:  0.9215686274509803\n",
      "- validation recall:  0.7811634349030471\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0322 - acc: 0.9898 - val_loss: 0.3716 - val_acc: 0.9218\n",
      "Epoch 81/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9871- train accuracy:  0.9972991222147198\n",
      "- train loss:  0.03339400812436416\n",
      "- train roc_auc: 0.9999524149559704\n",
      "- train precision:  0.9931199521387974\n",
      "- train recall:  0.9972964854310604\n",
      "- validation accuracy:  0.9157175398633257\n",
      "- validation loss:  0.3473228609236063\n",
      "- validation roc_auc: 0.9628530696925092\n",
      "- validation precision:  0.8551136363636364\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0332 - acc: 0.9873 - val_loss: 0.3473 - val_acc: 0.9157\n",
      "Epoch 82/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9848- train accuracy:  0.9983119513841998\n",
      "- train loss:  0.03931296866426314\n",
      "- train roc_auc: 0.9999771683467884\n",
      "- train precision:  0.997593984962406\n",
      "- train recall:  0.9963953139080806\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.3886609006398722\n",
      "- validation roc_auc: 0.9634702534799893\n",
      "- validation precision:  0.8984615384615384\n",
      "- validation recall:  0.8088642659279779\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0391 - acc: 0.9849 - val_loss: 0.3887 - val_acc: 0.9226\n",
      "Epoch 83/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9878- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.03764136878466693\n",
      "- train roc_auc: 0.9999939879584979\n",
      "- train precision:  0.9987973541791942\n",
      "- train recall:  0.9978972664463803\n",
      "- validation accuracy:  0.9271070615034168\n",
      "- validation loss:  0.3520804544516196\n",
      "- validation roc_auc: 0.9656586481067235\n",
      "- validation precision:  0.9076923076923077\n",
      "- validation recall:  0.817174515235457\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0375 - acc: 0.9877 - val_loss: 0.3521 - val_acc: 0.9271\n",
      "Epoch 84/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9860- train accuracy:  0.9988183659689399\n",
      "- train loss:  0.041161435783905485\n",
      "- train roc_auc: 0.9999844321623275\n",
      "- train precision:  0.9978972664463803\n",
      "- train recall:  0.9978972664463803\n",
      "- validation accuracy:  0.9233105542900532\n",
      "- validation loss:  0.3434646401834379\n",
      "- validation roc_auc: 0.9640468711969309\n",
      "- validation precision:  0.8823529411764706\n",
      "- validation recall:  0.8310249307479224\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0410 - acc: 0.9861 - val_loss: 0.3435 - val_acc: 0.9233\n",
      "Epoch 85/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9859- train accuracy:  0.9989871708305199\n",
      "- train loss:  0.040351580581605775\n",
      "- train roc_auc: 0.9999880993313376\n",
      "- train precision:  0.9978985289702792\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9210326499620349\n",
      "- validation loss:  0.35728514253323784\n",
      "- validation roc_auc: 0.9658245343594618\n",
      "- validation precision:  0.8640226628895185\n",
      "- validation recall:  0.8448753462603878\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0402 - acc: 0.9860 - val_loss: 0.3573 - val_acc: 0.9210\n",
      "Epoch 86/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9897- train accuracy:  0.9981431465226198\n",
      "- train loss:  0.030761192477873234\n",
      "- train roc_auc: 0.9999777325266361\n",
      "- train precision:  0.9937294714840251\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.3351149676238938\n",
      "- validation roc_auc: 0.9641606010732624\n",
      "- validation precision:  0.8405405405405405\n",
      "- validation recall:  0.8614958448753463\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0306 - acc: 0.9896 - val_loss: 0.3351 - val_acc: 0.9172\n",
      "Epoch 87/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9887- train accuracy:  0.9982275489534098\n",
      "- train loss:  0.031960288101051365\n",
      "- train roc_auc: 0.9999782614452433\n",
      "- train precision:  0.9963985594237695\n",
      "- train recall:  0.9972964854310604\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.3734142210959664\n",
      "- validation roc_auc: 0.9624952189988294\n",
      "- validation precision:  0.8753623188405797\n",
      "- validation recall:  0.8365650969529086\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0318 - acc: 0.9885 - val_loss: 0.3734 - val_acc: 0.9226\n",
      "Epoch 88/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9877- train accuracy:  0.9979743416610398\n",
      "- train loss:  0.03074118073373256\n",
      "- train roc_auc: 0.9999959273267245\n",
      "- train precision:  0.9931363772008356\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.3853057142270725\n",
      "- validation roc_auc: 0.9629783898747089\n",
      "- validation precision:  0.8267716535433071\n",
      "- validation recall:  0.8725761772853186\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0306 - acc: 0.9877 - val_loss: 0.3853 - val_acc: 0.9150\n",
      "Epoch 89/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9878- train accuracy:  0.9987339635381499\n",
      "- train loss:  0.03483653423910244\n",
      "- train roc_auc: 0.9999943053096623\n",
      "- train precision:  0.9961077844311377\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.36256987694863035\n",
      "- validation roc_auc: 0.9662193291530965\n",
      "- validation precision:  0.8489010989010989\n",
      "- validation recall:  0.8559556786703602\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0347 - acc: 0.9878 - val_loss: 0.3626 - val_acc: 0.9188\n",
      "Epoch 90/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9881- train accuracy:  0.9956954760297096\n",
      "- train loss:  0.03466813905091653\n",
      "- train roc_auc: 0.9999029081743348\n",
      "- train precision:  0.9877976190476191\n",
      "- train recall:  0.9969960949234005\n",
      "- validation accuracy:  0.9141989369779803\n",
      "- validation loss:  0.418076613090576\n",
      "- validation roc_auc: 0.9584777292272743\n",
      "- validation precision:  0.8387978142076503\n",
      "- validation recall:  0.850415512465374\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0368 - acc: 0.9881 - val_loss: 0.4181 - val_acc: 0.9142\n",
      "Epoch 91/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9840- train accuracy:  0.99966239027684\n",
      "- train loss:  0.042268650714121885\n",
      "- train roc_auc: 0.9999995239732535\n",
      "- train precision:  0.999099369558691\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.39583796783206854\n",
      "- validation roc_auc: 0.9640323833145956\n",
      "- validation precision:  0.8591549295774648\n",
      "- validation recall:  0.8448753462603878\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0425 - acc: 0.9841 - val_loss: 0.3958 - val_acc: 0.9195\n",
      "Epoch 92/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9921- train accuracy:  0.99966239027684\n",
      "- train loss:  0.023230605281117214\n",
      "- train roc_auc: 0.9999982193073557\n",
      "- train precision:  0.999099369558691\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9233105542900532\n",
      "- validation loss:  0.38608824808487135\n",
      "- validation roc_auc: 0.9656745847772923\n",
      "- validation precision:  0.867231638418079\n",
      "- validation recall:  0.850415512465374\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0231 - acc: 0.9921 - val_loss: 0.3861 - val_acc: 0.9233\n",
      "Epoch 93/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9896- train accuracy:  0.9987339635381499\n",
      "- train loss:  0.028926558943199978\n",
      "- train roc_auc: 0.9999939174360171\n",
      "- train precision:  0.9958108916816277\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.40279968068196\n",
      "- validation roc_auc: 0.9613506762943475\n",
      "- validation precision:  0.8526912181303116\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0296 - acc: 0.9894 - val_loss: 0.4028 - val_acc: 0.9150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9868- train accuracy:  0.9991559756920999\n",
      "- train loss:  0.03987850471258224\n",
      "- train roc_auc: 0.9999869357104016\n",
      "- train precision:  0.9990977443609023\n",
      "- train recall:  0.9978972664463803\n",
      "- validation accuracy:  0.9157175398633257\n",
      "- validation loss:  0.39976383228598894\n",
      "- validation roc_auc: 0.9574527115520579\n",
      "- validation precision:  0.8881987577639752\n",
      "- validation recall:  0.7922437673130194\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0397 - acc: 0.9866 - val_loss: 0.3998 - val_acc: 0.9157\n",
      "Epoch 95/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9905- train accuracy:  0.9988183659689399\n",
      "- train loss:  0.029383099276024113\n",
      "- train roc_auc: 0.9999985013972796\n",
      "- train precision:  0.9964061096136568\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.401494609060483\n",
      "- validation roc_auc: 0.9628211963513718\n",
      "- validation precision:  0.8436657681940701\n",
      "- validation recall:  0.8670360110803325\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0293 - acc: 0.9905 - val_loss: 0.4015 - val_acc: 0.9195\n",
      "Epoch 96/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9918- train accuracy:  0.9972991222147198\n",
      "- train loss:  0.022812033241155955\n",
      "- train roc_auc: 0.9999285430961646\n",
      "- train precision:  0.9928251121076234\n",
      "- train recall:  0.9975968759387204\n",
      "- validation accuracy:  0.9210326499620349\n",
      "- validation loss:  0.3746330177096046\n",
      "- validation roc_auc: 0.9593412070144531\n",
      "- validation precision:  0.8539944903581267\n",
      "- validation recall:  0.8587257617728532\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0227 - acc: 0.9919 - val_loss: 0.3746 - val_acc: 0.9210\n",
      "Epoch 97/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9884- train accuracy:  0.99957798784605\n",
      "- train loss:  0.03823654690523463\n",
      "- train roc_auc: 0.9999931416887263\n",
      "- train precision:  0.998500299940012\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9248291571753986\n",
      "- validation loss:  0.35560662468545984\n",
      "- validation roc_auc: 0.9656666164420079\n",
      "- validation precision:  0.8638888888888889\n",
      "- validation recall:  0.8614958448753463\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0381 - acc: 0.9885 - val_loss: 0.3556 - val_acc: 0.9248\n",
      "Epoch 98/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9861- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.03783035084902662\n",
      "- train roc_auc: 0.9999933532561693\n",
      "- train precision:  0.9987984379693602\n",
      "- train recall:  0.9987984379693602\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.41549101469638045\n",
      "- validation roc_auc: 0.962593736598709\n",
      "- validation precision:  0.8832335329341318\n",
      "- validation recall:  0.817174515235457\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0382 - acc: 0.9861 - val_loss: 0.4155 - val_acc: 0.9203\n",
      "Epoch 99/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9893- train accuracy:  0.99949358541526\n",
      "- train loss:  0.02699055975544531\n",
      "- train roc_auc: 0.9999934943011312\n",
      "- train precision:  0.9996992481203008\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9240698557327259\n",
      "- validation loss:  0.38890135340285104\n",
      "- validation roc_auc: 0.963199330080321\n",
      "- validation precision:  0.9169329073482428\n",
      "- validation recall:  0.7950138504155124\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0273 - acc: 0.9893 - val_loss: 0.3889 - val_acc: 0.9241\n",
      "Epoch 100/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9897- train accuracy:  0.9964550979068197\n",
      "- train loss:  0.03141244556214019\n",
      "- train roc_auc: 0.9999271326465454\n",
      "- train precision:  0.9975779594308205\n",
      "- train recall:  0.9897867227395615\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.3909530313339031\n",
      "- validation roc_auc: 0.9633116111684188\n",
      "- validation precision:  0.8975155279503105\n",
      "- validation recall:  0.8005540166204986\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0315 - acc: 0.9893 - val_loss: 0.3910 - val_acc: 0.9203\n",
      "Epoch 101/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9877- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.03438629199976949\n",
      "- train roc_auc: 0.9999831980189107\n",
      "- train precision:  0.9984975961538461\n",
      "- train recall:  0.9981976569540403\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.37723832358531906\n",
      "- validation roc_auc: 0.9660107036474693\n",
      "- validation precision:  0.8828828828828829\n",
      "- validation recall:  0.814404432132964\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0342 - acc: 0.9879 - val_loss: 0.3772 - val_acc: 0.9195\n",
      "Epoch 102/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9886- train accuracy:  0.99974679270763\n",
      "- train loss:  0.031350866637856675\n",
      "- train roc_auc: 0.9999985013972796\n",
      "- train precision:  0.9993993993993994\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.4473066724152301\n",
      "- validation roc_auc: 0.9613818252413682\n",
      "- validation precision:  0.8649425287356322\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0312 - acc: 0.9887 - val_loss: 0.4473 - val_acc: 0.9188\n",
      "Epoch 103/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9900- train accuracy:  0.9989027683997299\n",
      "- train loss:  0.02758986727997364\n",
      "- train roc_auc: 0.999979848201065\n",
      "- train precision:  0.9967046135410426\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.911921032649962\n",
      "- validation loss:  0.4158213947791445\n",
      "- validation roc_auc: 0.9655521621715597\n",
      "- validation precision:  0.8337874659400545\n",
      "- validation recall:  0.8476454293628809\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0275 - acc: 0.9899 - val_loss: 0.4158 - val_acc: 0.9119\n",
      "Epoch 104/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9907- train accuracy:  0.99957798784605\n",
      "- train loss:  0.029587706058710955\n",
      "- train roc_auc: 0.9999982545685961\n",
      "- train precision:  0.9993990384615384\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.40593636859612836\n",
      "- validation roc_auc: 0.9652892071071756\n",
      "- validation precision:  0.8771929824561403\n",
      "- validation recall:  0.8310249307479224\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0294 - acc: 0.9909 - val_loss: 0.4059 - val_acc: 0.9218\n",
      "Epoch 105/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9866- train accuracy:  0.9973835246455098\n",
      "- train loss:  0.039645750532436116\n",
      "- train roc_auc: 0.9999554826838923\n",
      "- train precision:  0.9948979591836735\n",
      "- train recall:  0.9957945328927605\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.3688341831744577\n",
      "- validation roc_auc: 0.9624800067223775\n",
      "- validation precision:  0.8587570621468926\n",
      "- validation recall:  0.8421052631578947\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0406 - acc: 0.9863 - val_loss: 0.3688 - val_acc: 0.9188\n",
      "Epoch 106/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9867- train accuracy:  0.9992403781228899\n",
      "- train loss:  0.03893706252052311\n",
      "- train roc_auc: 0.9999901444832855\n",
      "- train precision:  0.9979004199160167\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.35115017217492733\n",
      "- validation roc_auc: 0.965484793518701\n",
      "- validation precision:  0.8636363636363636\n",
      "- validation recall:  0.8421052631578947\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0389 - acc: 0.9868 - val_loss: 0.3512 - val_acc: 0.9203\n",
      "Epoch 107/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9883- train accuracy:  0.99957798784605\n",
      "- train loss:  0.03543810080170913\n",
      "- train roc_auc: 0.9999985013972796\n",
      "- train precision:  0.9987995198079231\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9134396355353075\n",
      "- validation loss:  0.3939276119027974\n",
      "- validation roc_auc: 0.9613318420473116\n",
      "- validation precision:  0.8538681948424068\n",
      "- validation recall:  0.8254847645429363\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0356 - acc: 0.9883 - val_loss: 0.3939 - val_acc: 0.9134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9923- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.02337279462412828\n",
      "- train roc_auc: 0.9999987129647224\n",
      "- train precision:  0.9981998199819982\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.41443424715449073\n",
      "- validation roc_auc: 0.9658933518005539\n",
      "- validation precision:  0.8418230563002681\n",
      "- validation recall:  0.8698060941828255\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0233 - acc: 0.9923 - val_loss: 0.4144 - val_acc: 0.9195\n",
      "Epoch 109/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9910- train accuracy:  0.9992403781228899\n",
      "- train loss:  0.027312631300503307\n",
      "- train roc_auc: 0.9999965620290531\n",
      "- train precision:  0.9993983152827918\n",
      "- train recall:  0.9978972664463803\n",
      "- validation accuracy:  0.9179954441913439\n",
      "- validation loss:  0.4058945159637847\n",
      "- validation roc_auc: 0.9640584615027992\n",
      "- validation precision:  0.9041533546325878\n",
      "- validation recall:  0.7839335180055401\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0272 - acc: 0.9909 - val_loss: 0.4059 - val_acc: 0.9180\n",
      "Epoch 110/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9894- train accuracy:  0.99957798784605\n",
      "- train loss:  0.029091167482081452\n",
      "- train roc_auc: 0.9999935648236121\n",
      "- train precision:  0.9987995198079231\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.38974544469298245\n",
      "- validation roc_auc: 0.963534724556381\n",
      "- validation precision:  0.867816091954023\n",
      "- validation recall:  0.8365650969529086\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0290 - acc: 0.9895 - val_loss: 0.3897 - val_acc: 0.9203\n",
      "Epoch 111/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9888- train accuracy:  0.99957798784605\n",
      "- train loss:  0.0326133815139019\n",
      "- train roc_auc: 0.9999976198662675\n",
      "- train precision:  0.998500299940012\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.3753017141408059\n",
      "- validation roc_auc: 0.9654355347187612\n",
      "- validation precision:  0.8735294117647059\n",
      "- validation recall:  0.8227146814404432\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0325 - acc: 0.9887 - val_loss: 0.3753 - val_acc: 0.9188\n",
      "Epoch 112/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9878- train accuracy:  0.9968771100607697\n",
      "- train loss:  0.03431152836031329\n",
      "- train roc_auc: 0.9999322102651748\n",
      "- train precision:  0.9945913461538461\n",
      "- train recall:  0.9942925803544608\n",
      "- validation accuracy:  0.9073652239939256\n",
      "- validation loss:  0.40149648373291724\n",
      "- validation roc_auc: 0.9597454189316056\n",
      "- validation precision:  0.8588588588588588\n",
      "- validation recall:  0.7922437673130194\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0371 - acc: 0.9875 - val_loss: 0.4015 - val_acc: 0.9074\n",
      "Epoch 113/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9876- train accuracy:  0.99957798784605\n",
      "- train loss:  0.03601326477523028\n",
      "- train roc_auc: 0.9999871120166041\n",
      "- train precision:  0.9987995198079231\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.4363667921035147\n",
      "- validation roc_auc: 0.961981623570046\n",
      "- validation precision:  0.8547486033519553\n",
      "- validation recall:  0.8476454293628809\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0358 - acc: 0.9877 - val_loss: 0.4364 - val_acc: 0.9188\n",
      "Epoch 114/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9875- train accuracy:  0.99957798784605\n",
      "- train loss:  0.035193646192048525\n",
      "- train roc_auc: 0.9999969851639388\n",
      "- train precision:  0.9990990990990991\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.40742377888187825\n",
      "- validation roc_auc: 0.9637976796207652\n",
      "- validation precision:  0.8794117647058823\n",
      "- validation recall:  0.8282548476454293\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0350 - acc: 0.9872 - val_loss: 0.4074 - val_acc: 0.9218\n",
      "Epoch 115/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9895- train accuracy:  0.99966239027684\n",
      "- train loss:  0.028832824404944613\n",
      "- train roc_auc: 0.9999954689305981\n",
      "- train precision:  0.9996994289149383\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9126803340926348\n",
      "- validation loss:  0.41214207183617757\n",
      "- validation roc_auc: 0.9604292469778277\n",
      "- validation precision:  0.875\n",
      "- validation recall:  0.7950138504155124\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0287 - acc: 0.9895 - val_loss: 0.4121 - val_acc: 0.9127\n",
      "Epoch 116/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9918- train accuracy:  0.9978899392302498\n",
      "- train loss:  0.023570966397675922\n",
      "- train roc_auc: 0.9999681414692252\n",
      "- train precision:  0.9981905910735827\n",
      "- train recall:  0.9942925803544608\n",
      "- validation accuracy:  0.9179954441913439\n",
      "- validation loss:  0.4440700008096166\n",
      "- validation roc_auc: 0.9576185978047961\n",
      "- validation precision:  0.896551724137931\n",
      "- validation recall:  0.7922437673130194\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0235 - acc: 0.9917 - val_loss: 0.4441 - val_acc: 0.9180\n",
      "Epoch 117/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9926- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.022686498403519873\n",
      "- train roc_auc: 0.9999927185538406\n",
      "- train precision:  0.9990982867448152\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.4095199678373228\n",
      "- validation roc_auc: 0.9639273461676654\n",
      "- validation precision:  0.8848484848484849\n",
      "- validation recall:  0.8088642659279779\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0226 - acc: 0.9927 - val_loss: 0.4095 - val_acc: 0.9188\n",
      "Epoch 118/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9894- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.03238162604395796\n",
      "- train roc_auc: 0.9999982898298365\n",
      "- train precision:  0.9993984962406015\n",
      "- train recall:  0.9981976569540403\n",
      "- validation accuracy:  0.9141989369779803\n",
      "- validation loss:  0.4874611146176685\n",
      "- validation roc_auc: 0.9581915935511538\n",
      "- validation precision:  0.8924050632911392\n",
      "- validation recall:  0.7811634349030471\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0322 - acc: 0.9891 - val_loss: 0.4875 - val_acc: 0.9142\n",
      "Epoch 119/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9883- train accuracy:  0.9994091829844699\n",
      "- train loss:  0.030474832307174425\n",
      "- train roc_auc: 0.9999969851639388\n",
      "- train precision:  0.9993986770895971\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.4245479207375598\n",
      "- validation roc_auc: 0.9641352472791758\n",
      "- validation precision:  0.8913043478260869\n",
      "- validation recall:  0.7950138504155124\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0303 - acc: 0.9885 - val_loss: 0.4245 - val_acc: 0.9172\n",
      "Epoch 120/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9916- train accuracy:  0.99949358541526\n",
      "- train loss:  0.025606253347765402\n",
      "- train roc_auc: 0.999987253061566\n",
      "- train precision:  0.9987991594115881\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9164768413059985\n",
      "- validation loss:  0.37187725620099604\n",
      "- validation roc_auc: 0.9603597051426187\n",
      "- validation precision:  0.8555240793201133\n",
      "- validation recall:  0.8365650969529086\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0255 - acc: 0.9916 - val_loss: 0.3719 - val_acc: 0.9165\n",
      "Epoch 121/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9929- train accuracy:  0.99966239027684\n",
      "- train loss:  0.020031054577710344\n",
      "- train roc_auc: 0.9999993829282916\n",
      "- train precision:  0.999099369558691\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9179954441913439\n",
      "- validation loss:  0.4262857625859202\n",
      "- validation roc_auc: 0.9650059690075221\n",
      "- validation precision:  0.8709677419354839\n",
      "- validation recall:  0.8227146814404432\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0199 - acc: 0.9929 - val_loss: 0.4263 - val_acc: 0.9180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9918- train accuracy:  0.9972991222147198\n",
      "- train loss:  0.02516040743380039\n",
      "- train roc_auc: 0.9999523091722491\n",
      "- train precision:  0.9966857487194938\n",
      "- train recall:  0.9936917993391409\n",
      "- validation accuracy:  0.9096431283219438\n",
      "- validation loss:  0.46127986152152173\n",
      "- validation roc_auc: 0.9537409160977758\n",
      "- validation precision:  0.8666666666666667\n",
      "- validation recall:  0.7922437673130194\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0250 - acc: 0.9917 - val_loss: 0.4613 - val_acc: 0.9096\n",
      "Epoch 123/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9898- train accuracy:  0.9989871708305199\n",
      "- train loss:  0.028602209169503144\n",
      "- train roc_auc: 0.9999930359050049\n",
      "- train precision:  0.9990972013241047\n",
      "- train recall:  0.9972964854310604\n",
      "- validation accuracy:  0.9134396355353075\n",
      "- validation loss:  0.47511053809453435\n",
      "- validation roc_auc: 0.9581206029277113\n",
      "- validation precision:  0.8920634920634921\n",
      "- validation recall:  0.778393351800554\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0285 - acc: 0.9898 - val_loss: 0.4751 - val_acc: 0.9134\n",
      "Epoch 124/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9930- train accuracy:  0.99949358541526\n",
      "- train loss:  0.021007744054992953\n",
      "- train roc_auc: 0.9999963857228507\n",
      "- train precision:  0.9982008995502248\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.438395409622062\n",
      "- validation roc_auc: 0.9603988224249238\n",
      "- validation precision:  0.8649425287356322\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0209 - acc: 0.9929 - val_loss: 0.4384 - val_acc: 0.9188\n",
      "Epoch 125/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9918- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.023130190337581256\n",
      "- train roc_auc: 0.9999980782623938\n",
      "- train precision:  0.9981998199819982\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.43548756254018034\n",
      "- validation roc_auc: 0.9647596750078233\n",
      "- validation precision:  0.8432432432432433\n",
      "- validation recall:  0.8642659279778393\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0230 - acc: 0.9916 - val_loss: 0.4355 - val_acc: 0.9188\n",
      "Epoch 126/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9830- train accuracy:  0.9994091829844699\n",
      "- train loss:  0.05038873421042091\n",
      "- train roc_auc: 0.9999935295623716\n",
      "- train precision:  0.9990985576923077\n",
      "- train recall:  0.9987984379693602\n",
      "- validation accuracy:  0.9126803340926348\n",
      "- validation loss:  0.44129600099251864\n",
      "- validation roc_auc: 0.9580612026101368\n",
      "- validation precision:  0.8796296296296297\n",
      "- validation recall:  0.7894736842105263\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0517 - acc: 0.9829 - val_loss: 0.4413 - val_acc: 0.9127\n",
      "Epoch 127/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9880- train accuracy:  0.9988183659689399\n",
      "- train loss:  0.03508299882058369\n",
      "- train roc_auc: 0.9999963504616101\n",
      "- train precision:  0.9964061096136568\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.911921032649962\n",
      "- validation loss:  0.45676399458513717\n",
      "- validation roc_auc: 0.9620584093464226\n",
      "- validation precision:  0.8249336870026526\n",
      "- validation recall:  0.8614958448753463\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0349 - acc: 0.9877 - val_loss: 0.4568 - val_acc: 0.9119\n",
      "Epoch 128/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9889- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.03297760909087428\n",
      "- train roc_auc: 0.9999925422476381\n",
      "- train precision:  0.9981981981981982\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9134396355353075\n",
      "- validation loss:  0.4320500270833368\n",
      "- validation roc_auc: 0.963634690944494\n",
      "- validation precision:  0.8538681948424068\n",
      "- validation recall:  0.8254847645429363\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0328 - acc: 0.9890 - val_loss: 0.4321 - val_acc: 0.9134\n",
      "Epoch 129/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9921- train accuracy:  0.9986495611073599\n",
      "- train loss:  0.022380949398924033\n",
      "- train roc_auc: 0.9999976903887485\n",
      "- train precision:  0.9955130122644331\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9134396355353075\n",
      "- validation loss:  0.5125525114476545\n",
      "- validation roc_auc: 0.9637353817267238\n",
      "- validation precision:  0.8328840970350404\n",
      "- validation recall:  0.8559556786703602\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0223 - acc: 0.9921 - val_loss: 0.5126 - val_acc: 0.9134\n",
      "Epoch 130/500\n",
      "181/186 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9906- train accuracy:  0.99949358541526\n",
      "- train loss:  0.027924384112559656\n",
      "- train roc_auc: 0.9999891571685521\n",
      "- train precision:  0.9982008995502248\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.43502586502963697\n",
      "- validation roc_auc: 0.9625749023516731\n",
      "- validation precision:  0.8442622950819673\n",
      "- validation recall:  0.8559556786703602\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0278 - acc: 0.9904 - val_loss: 0.4350 - val_acc: 0.9172\n",
      "Epoch 131/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9923- train accuracy:  0.9994091829844699\n",
      "- train loss:  0.020315904317655863\n",
      "- train roc_auc: 0.9999953984081172\n",
      "- train precision:  0.9979016786570744\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.478735189322006\n",
      "- validation roc_auc: 0.9623677256342795\n",
      "- validation precision:  0.8448753462603878\n",
      "- validation recall:  0.8448753462603878\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0202 - acc: 0.9924 - val_loss: 0.4787 - val_acc: 0.9157\n",
      "Epoch 132/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9909- train accuracy:  0.99974679270763\n",
      "- train loss:  0.029166042560568126\n",
      "- train roc_auc: 0.9999932827336884\n",
      "- train precision:  0.9993993993993994\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9126803340926348\n",
      "- validation loss:  0.4983295262448246\n",
      "- validation roc_auc: 0.9624495821694734\n",
      "- validation precision:  0.8575581395348837\n",
      "- validation recall:  0.817174515235457\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0290 - acc: 0.9909 - val_loss: 0.4983 - val_acc: 0.9127\n",
      "Epoch 133/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9895- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.03137679988280024\n",
      "- train roc_auc: 0.9999924717251572\n",
      "- train precision:  0.9979010494752624\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.41376383119491167\n",
      "- validation roc_auc: 0.9624742115694433\n",
      "- validation precision:  0.8653295128939829\n",
      "- validation recall:  0.8365650969529086\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0312 - acc: 0.9896 - val_loss: 0.4138 - val_acc: 0.9195\n",
      "Epoch 134/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9925- train accuracy:  0.9992403781228899\n",
      "- train loss:  0.021452296890757328\n",
      "- train roc_auc: 0.9999914138879429\n",
      "- train precision:  0.9984984984984985\n",
      "- train recall:  0.9987984379693602\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.4147194411461337\n",
      "- validation roc_auc: 0.9595788082847506\n",
      "- validation precision:  0.878419452887538\n",
      "- validation recall:  0.8005540166204986\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0214 - acc: 0.9924 - val_loss: 0.4147 - val_acc: 0.9150\n",
      "Epoch 135/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9900- train accuracy:  0.9984807562457799\n",
      "- train loss:  0.02945344286227497\n",
      "- train roc_auc: 0.9999915196716642\n",
      "- train precision:  0.9981944026482095\n",
      "- train recall:  0.9963953139080806\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.4480315229615752\n",
      "- validation roc_auc: 0.9568536376174968\n",
      "- validation precision:  0.8738738738738738\n",
      "- validation recall:  0.8060941828254847\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0293 - acc: 0.9897 - val_loss: 0.4480 - val_acc: 0.9150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9896- train accuracy:  0.99966239027684\n",
      "- train loss:  0.02805172352973626\n",
      "- train roc_auc: 0.9999947637057885\n",
      "- train precision:  0.9993992189846801\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.42981017011535283\n",
      "- validation roc_auc: 0.9622539957579481\n",
      "- validation precision:  0.8825301204819277\n",
      "- validation recall:  0.8116343490304709\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0279 - acc: 0.9896 - val_loss: 0.4298 - val_acc: 0.9188\n",
      "Epoch 137/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9911- train accuracy:  0.99974679270763\n",
      "- train loss:  0.022913804601283894\n",
      "- train roc_auc: 0.9999948342282695\n",
      "- train precision:  0.9993993993993994\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.42158331353735\n",
      "- validation roc_auc: 0.9672103003048251\n",
      "- validation precision:  0.8627450980392157\n",
      "- validation recall:  0.853185595567867\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0228 - acc: 0.9911 - val_loss: 0.4216 - val_acc: 0.9226\n",
      "Epoch 138/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9908- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.028706417905036575\n",
      "- train roc_auc: 0.9999904265732094\n",
      "- train precision:  0.9990982867448152\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9141989369779803\n",
      "- validation loss:  0.4795732005423638\n",
      "- validation roc_auc: 0.9605907868658654\n",
      "- validation precision:  0.8647058823529412\n",
      "- validation recall:  0.814404432132964\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0286 - acc: 0.9908 - val_loss: 0.4796 - val_acc: 0.9142\n",
      "Epoch 139/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9924- train accuracy:  0.99966239027684\n",
      "- train loss:  0.021017632248781715\n",
      "- train roc_auc: 0.9999986777034819\n",
      "- train precision:  0.9993992189846801\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.932422171602126\n",
      "- validation loss:  0.43343148877789056\n",
      "- validation roc_auc: 0.9637904356795975\n",
      "- validation precision:  0.9096385542168675\n",
      "- validation recall:  0.8365650969529086\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0209 - acc: 0.9924 - val_loss: 0.4334 - val_acc: 0.9324\n",
      "Epoch 140/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9899- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.02927668998672301\n",
      "- train roc_auc: 0.9999959978492053\n",
      "- train precision:  0.9990982867448152\n",
      "- train recall:  0.9984980474617002\n",
      "- validation accuracy:  0.9248291571753986\n",
      "- validation loss:  0.45015388138835505\n",
      "- validation roc_auc: 0.9637534915796429\n",
      "- validation precision:  0.901840490797546\n",
      "- validation recall:  0.814404432132964\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0291 - acc: 0.9894 - val_loss: 0.4502 - val_acc: 0.9248\n",
      "Epoch 141/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9887- train accuracy:  0.9983119513841998\n",
      "- train loss:  0.034707374321479176\n",
      "- train roc_auc: 0.9999912023205\n",
      "- train precision:  0.9990950226244344\n",
      "- train recall:  0.9948933613697807\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.4937207819208016\n",
      "- validation roc_auc: 0.9613767544825508\n",
      "- validation precision:  0.9150326797385621\n",
      "- validation recall:  0.775623268698061\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0348 - acc: 0.9887 - val_loss: 0.4937 - val_acc: 0.9188\n",
      "Epoch 142/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9893- train accuracy:  0.99957798784605\n",
      "- train loss:  0.03132569514887886\n",
      "- train roc_auc: 0.9999992066220892\n",
      "- train precision:  0.9996993385447985\n",
      "- train recall:  0.9987984379693602\n",
      "- validation accuracy:  0.9240698557327259\n",
      "- validation loss:  0.43516958427411095\n",
      "- validation roc_auc: 0.9658585808829495\n",
      "- validation precision:  0.8942598187311178\n",
      "- validation recall:  0.8199445983379502\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0312 - acc: 0.9893 - val_loss: 0.4352 - val_acc: 0.9241\n",
      "Epoch 143/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9914- train accuracy:  0.99949358541526\n",
      "- train loss:  0.024316742668490526\n",
      "- train roc_auc: 0.9999989245321652\n",
      "- train precision:  0.9982008995502248\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.3933995895161263\n",
      "- validation roc_auc: 0.9620127725170667\n",
      "- validation precision:  0.869942196531792\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0242 - acc: 0.9916 - val_loss: 0.3934 - val_acc: 0.9203\n",
      "Epoch 144/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9924- train accuracy:  0.99949358541526\n",
      "- train loss:  0.021993961376779916\n",
      "- train roc_auc: 0.9999952926243958\n",
      "- train precision:  0.9982008995502248\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9134396355353075\n",
      "- validation loss:  0.45188622874750184\n",
      "- validation roc_auc: 0.9601967164663476\n",
      "- validation precision:  0.8708708708708709\n",
      "- validation recall:  0.8033240997229917\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0219 - acc: 0.9925 - val_loss: 0.4519 - val_acc: 0.9134\n",
      "Epoch 145/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9913- train accuracy:  0.99957798784605\n",
      "- train loss:  0.026228580462890092\n",
      "- train roc_auc: 0.9999993476670511\n",
      "- train precision:  0.9990990990990991\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9233105542900532\n",
      "- validation loss:  0.42445671508596444\n",
      "- validation roc_auc: 0.9626611052515676\n",
      "- validation precision:  0.8693181818181818\n",
      "- validation recall:  0.8476454293628809\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0261 - acc: 0.9913 - val_loss: 0.4245 - val_acc: 0.9233\n",
      "Epoch 146/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9932- train accuracy:  0.99949358541526\n",
      "- train loss:  0.020058187022619703\n",
      "- train roc_auc: 0.9999974082988247\n",
      "- train precision:  0.9982008995502248\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9179954441913439\n",
      "- validation loss:  0.46084507152539994\n",
      "- validation roc_auc: 0.9616295680293003\n",
      "- validation precision:  0.8709677419354839\n",
      "- validation recall:  0.8227146814404432\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0200 - acc: 0.9933 - val_loss: 0.4608 - val_acc: 0.9180\n",
      "Epoch 147/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9935- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.02013130774291242\n",
      "- train roc_auc: 0.9999978666949509\n",
      "- train precision:  0.9981998199819982\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.48376209874359516\n",
      "- validation roc_auc: 0.9642308673025881\n",
      "- validation precision:  0.8587570621468926\n",
      "- validation recall:  0.8421052631578947\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0200 - acc: 0.9934 - val_loss: 0.4838 - val_acc: 0.9188\n",
      "Epoch 148/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9907- train accuracy:  0.99966239027684\n",
      "- train loss:  0.028210151875105477\n",
      "- train roc_auc: 0.9999979019561914\n",
      "- train precision:  0.9993992189846801\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.43510376891133995\n",
      "- validation roc_auc: 0.9650588497780457\n",
      "- validation precision:  0.8685714285714285\n",
      "- validation recall:  0.8421052631578947\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0281 - acc: 0.9907 - val_loss: 0.4351 - val_acc: 0.9218\n",
      "Epoch 149/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9914- train accuracy:  0.99957798784605\n",
      "- train loss:  0.022821315845103503\n",
      "- train roc_auc: 0.9999996297569749\n",
      "- train precision:  0.9990990990990991\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.44883750310067255\n",
      "- validation roc_auc: 0.964355463090671\n",
      "- validation precision:  0.8662790697674418\n",
      "- validation recall:  0.8254847645429363\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0227 - acc: 0.9914 - val_loss: 0.4488 - val_acc: 0.9172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9911- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.025703184636098136\n",
      "- train roc_auc: 0.9999941995259409\n",
      "- train precision:  0.9978991596638656\n",
      "- train recall:  0.9987984379693602\n",
      "- validation accuracy:  0.9278663629460896\n",
      "- validation loss:  0.45050770600998863\n",
      "- validation roc_auc: 0.9628813210630629\n",
      "- validation precision:  0.884393063583815\n",
      "- validation recall:  0.8476454293628809\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0256 - acc: 0.9908 - val_loss: 0.4505 - val_acc: 0.9279\n",
      "Epoch 151/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9891- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.031147291827740953\n",
      "- train roc_auc: 0.9999962799391292\n",
      "- train precision:  0.9981998199819982\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.3993031695292769\n",
      "- validation roc_auc: 0.965651404165556\n",
      "- validation precision:  0.845108695652174\n",
      "- validation recall:  0.8614958448753463\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0310 - acc: 0.9892 - val_loss: 0.3993 - val_acc: 0.9188\n",
      "Epoch 152/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9917- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.0242842031271932\n",
      "- train roc_auc: 0.9999995239732535\n",
      "- train precision:  0.9967065868263473\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.43066782510543106\n",
      "- validation roc_auc: 0.964409792649428\n",
      "- validation precision:  0.8493150684931506\n",
      "- validation recall:  0.8587257617728532\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0242 - acc: 0.9917 - val_loss: 0.4307 - val_acc: 0.9195\n",
      "Epoch 153/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9882- train accuracy:  0.99957798784605\n",
      "- train loss:  0.03374023536713209\n",
      "- train roc_auc: 0.9999988892709248\n",
      "- train precision:  0.9990990990990991\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9179954441913439\n",
      "- validation loss:  0.40425286842008024\n",
      "- validation roc_auc: 0.9626966005632889\n",
      "- validation precision:  0.8624641833810889\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0347 - acc: 0.9883 - val_loss: 0.4043 - val_acc: 0.9180\n",
      "Epoch 154/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9919- train accuracy:  0.9989027683997299\n",
      "- train loss:  0.02553258647968971\n",
      "- train roc_auc: 0.9999942700484218\n",
      "- train precision:  0.9981971153846154\n",
      "- train recall:  0.9978972664463803\n",
      "- validation accuracy:  0.9126803340926348\n",
      "- validation loss:  0.43131680149604806\n",
      "- validation roc_auc: 0.9579018359044496\n",
      "- validation precision:  0.8660714285714286\n",
      "- validation recall:  0.8060941828254847\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0255 - acc: 0.9919 - val_loss: 0.4313 - val_acc: 0.9127\n",
      "Epoch 155/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9886- train accuracy:  0.99957798784605\n",
      "- train loss:  0.03231791584126168\n",
      "- train roc_auc: 0.9999981487848748\n",
      "- train precision:  0.9987995198079231\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.44924926868694337\n",
      "- validation roc_auc: 0.9592064697087356\n",
      "- validation precision:  0.8783382789317508\n",
      "- validation recall:  0.8199445983379502\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0326 - acc: 0.9886 - val_loss: 0.4492 - val_acc: 0.9195\n",
      "Epoch 156/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9905- train accuracy:  0.9992403781228899\n",
      "- train loss:  0.027670252259078973\n",
      "- train roc_auc: 0.9999978314337103\n",
      "- train precision:  0.9984984984984985\n",
      "- train recall:  0.9987984379693602\n",
      "- validation accuracy:  0.9301442672741078\n",
      "- validation loss:  0.4118980007725587\n",
      "- validation roc_auc: 0.9671965368166066\n",
      "- validation precision:  0.8991097922848664\n",
      "- validation recall:  0.8393351800554016\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0275 - acc: 0.9904 - val_loss: 0.4119 - val_acc: 0.9301\n",
      "Epoch 157/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9906- train accuracy:  0.9970459149223497\n",
      "- train loss:  0.026079315612479986\n",
      "- train roc_auc: 0.9999819286142534\n",
      "- train precision:  0.9898869720404521\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9179954441913439\n",
      "- validation loss:  0.39486627594123036\n",
      "- validation roc_auc: 0.9585364051507319\n",
      "- validation precision:  0.8543417366946778\n",
      "- validation recall:  0.8448753462603878\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0260 - acc: 0.9906 - val_loss: 0.3949 - val_acc: 0.9180\n",
      "Epoch 158/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9920- train accuracy:  0.99966239027684\n",
      "- train loss:  0.02361853563084758\n",
      "- train roc_auc: 0.9999996297569749\n",
      "- train precision:  0.9987998799879988\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.4231470953178116\n",
      "- validation roc_auc: 0.9603727442367204\n",
      "- validation precision:  0.8615819209039548\n",
      "- validation recall:  0.8448753462603878\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0235 - acc: 0.9920 - val_loss: 0.4231 - val_acc: 0.9203\n",
      "Epoch 159/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9921- train accuracy:  0.9989871708305199\n",
      "- train loss:  0.021024763100180154\n",
      "- train roc_auc: 0.999996632551534\n",
      "- train precision:  0.9967056004791854\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9126803340926348\n",
      "- validation loss:  0.4229933910960036\n",
      "- validation roc_auc: 0.9593028141262647\n",
      "- validation precision:  0.8342391304347826\n",
      "- validation recall:  0.850415512465374\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0210 - acc: 0.9917 - val_loss: 0.4230 - val_acc: 0.9127\n",
      "Epoch 160/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9892- train accuracy:  0.9994091829844699\n",
      "- train loss:  0.029745104954605337\n",
      "- train roc_auc: 0.9999937411298145\n",
      "- train precision:  0.998499399759904\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.44865402952685174\n",
      "- validation roc_auc: 0.9566689171177227\n",
      "- validation precision:  0.8742690058479532\n",
      "- validation recall:  0.8282548476454293\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0303 - acc: 0.9893 - val_loss: 0.4487 - val_acc: 0.9203\n",
      "Epoch 161/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9900- train accuracy:  0.99966239027684\n",
      "- train loss:  0.03036126884344121\n",
      "- train roc_auc: 0.9999993124058106\n",
      "- train precision:  0.999099369558691\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.47890218571903315\n",
      "- validation roc_auc: 0.9609261813419256\n",
      "- validation precision:  0.8469945355191257\n",
      "- validation recall:  0.8587257617728532\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0302 - acc: 0.9902 - val_loss: 0.4789 - val_acc: 0.9188\n",
      "Epoch 162/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9913- train accuracy:  0.9989871708305199\n",
      "- train loss:  0.03018355498650532\n",
      "- train roc_auc: 0.9999926127701193\n",
      "- train precision:  0.9970032963739887\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.48371317844637074\n",
      "- validation roc_auc: 0.9583314016156886\n",
      "- validation precision:  0.8841463414634146\n",
      "- validation recall:  0.8033240997229917\n",
      "186/186 [==============================] - 4s 20ms/step - loss: 0.0300 - acc: 0.9912 - val_loss: 0.4837 - val_acc: 0.9172\n",
      "Epoch 163/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9905- train accuracy:  0.9966239027683997\n",
      "- train loss:  0.02801761279470752\n",
      "- train roc_auc: 0.9999350311644133\n",
      "- train precision:  0.9919234220759796\n",
      "- train recall:  0.9960949234004205\n",
      "- validation accuracy:  0.9164768413059985\n",
      "- validation loss:  0.48618396924315027\n",
      "- validation roc_auc: 0.9540973180032222\n",
      "- validation precision:  0.8555240793201133\n",
      "- validation recall:  0.8365650969529086\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0291 - acc: 0.9905 - val_loss: 0.4862 - val_acc: 0.9165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9899- train accuracy:  0.99966239027684\n",
      "- train loss:  0.03158255067120534\n",
      "- train roc_auc: 0.9999957862817626\n",
      "- train precision:  0.9996994289149383\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.4740043490242578\n",
      "- validation roc_auc: 0.9621670684639367\n",
      "- validation precision:  0.8567335243553008\n",
      "- validation recall:  0.8282548476454293\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0315 - acc: 0.9899 - val_loss: 0.4740 - val_acc: 0.9150\n",
      "Epoch 165/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9888- train accuracy:  0.99966239027684\n",
      "- train loss:  0.037454720041799505\n",
      "- train roc_auc: 0.9999986777034819\n",
      "- train precision:  0.9987998799879988\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9233105542900532\n",
      "- validation loss:  0.48380841155258564\n",
      "- validation roc_auc: 0.9574498139755908\n",
      "- validation precision:  0.8892215568862275\n",
      "- validation recall:  0.8227146814404432\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0374 - acc: 0.9888 - val_loss: 0.4838 - val_acc: 0.9233\n",
      "Epoch 166/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9936- train accuracy:  0.99974679270763\n",
      "- train loss:  0.018618341757470062\n",
      "- train roc_auc: 0.9999991008383677\n",
      "- train precision:  0.9996995192307693\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.48960282760491947\n",
      "- validation roc_auc: 0.9583806604156284\n",
      "- validation precision:  0.9059561128526645\n",
      "- validation recall:  0.8005540166204986\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0185 - acc: 0.9935 - val_loss: 0.4896 - val_acc: 0.9226\n",
      "Epoch 167/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9909- train accuracy:  0.99949358541526\n",
      "- train loss:  0.025784439383530398\n",
      "- train roc_auc: 0.9999981840461152\n",
      "- train precision:  0.9982008995502248\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9233105542900532\n",
      "- validation loss:  0.47487158465765605\n",
      "- validation roc_auc: 0.9606067235364342\n",
      "- validation precision:  0.877906976744186\n",
      "- validation recall:  0.8365650969529086\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0257 - acc: 0.9909 - val_loss: 0.4749 - val_acc: 0.9233\n",
      "Epoch 168/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9934- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.020066627914362364\n",
      "- train roc_auc: 0.9999984308747987\n",
      "- train precision:  0.9973021582733813\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9134396355353075\n",
      "- validation loss:  0.505526732701466\n",
      "- validation roc_auc: 0.9580372976042837\n",
      "- validation precision:  0.8498583569405099\n",
      "- validation recall:  0.8310249307479224\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0200 - acc: 0.9934 - val_loss: 0.5055 - val_acc: 0.9134\n",
      "Epoch 169/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9938- train accuracy:  0.99966239027684\n",
      "- train loss:  0.020726083729168208\n",
      "- train roc_auc: 0.9999996650182155\n",
      "- train precision:  0.9993992189846801\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9187547456340167\n",
      "- validation loss:  0.4649304652087328\n",
      "- validation roc_auc: 0.961024698941805\n",
      "- validation precision:  0.8735294117647059\n",
      "- validation recall:  0.8227146814404432\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0207 - acc: 0.9934 - val_loss: 0.4649 - val_acc: 0.9188\n",
      "Epoch 170/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9923- train accuracy:  0.99957798784605\n",
      "- train loss:  0.024287496324770712\n",
      "- train roc_auc: 0.9999965972902936\n",
      "- train precision:  0.9993990384615384\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.531429171471679\n",
      "- validation roc_auc: 0.9549825276139037\n",
      "- validation precision:  0.9003215434083601\n",
      "- validation recall:  0.775623268698061\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0243 - acc: 0.9923 - val_loss: 0.5314 - val_acc: 0.9150\n",
      "Epoch 171/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9918- train accuracy:  0.9982275489534098\n",
      "- train loss:  0.02229942020676405\n",
      "- train roc_auc: 0.9999523444334896\n",
      "- train precision:  0.99789283564118\n",
      "- train recall:  0.9957945328927605\n",
      "- validation accuracy:  0.9157175398633257\n",
      "- validation loss:  0.45192550155549854\n",
      "- validation roc_auc: 0.9590123320854438\n",
      "- validation precision:  0.8810975609756098\n",
      "- validation recall:  0.8005540166204986\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0222 - acc: 0.9919 - val_loss: 0.4519 - val_acc: 0.9157\n",
      "Epoch 172/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9935- train accuracy:  0.99949358541526\n",
      "- train loss:  0.017701437559725867\n",
      "- train roc_auc: 0.9999991713608487\n",
      "- train precision:  0.9987991594115881\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.914958238420653\n",
      "- validation loss:  0.5115785463886362\n",
      "- validation roc_auc: 0.9648661609429872\n",
      "- validation precision:  0.832\n",
      "- validation recall:  0.8642659279778393\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0176 - acc: 0.9934 - val_loss: 0.5116 - val_acc: 0.9150\n",
      "Epoch 173/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9929- train accuracy:  0.99966239027684\n",
      "- train loss:  0.02331420497080232\n",
      "- train roc_auc: 0.9999997002794558\n",
      "- train precision:  0.9996994289149383\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9157175398633257\n",
      "- validation loss:  0.4897614807546727\n",
      "- validation roc_auc: 0.9607320437186337\n",
      "- validation precision:  0.8858024691358025\n",
      "- validation recall:  0.7950138504155124\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0232 - acc: 0.9929 - val_loss: 0.4898 - val_acc: 0.9157\n",
      "Epoch 174/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9929- train accuracy:  0.99949358541526\n",
      "- train loss:  0.021250480204635828\n",
      "- train roc_auc: 0.9999976198662676\n",
      "- train precision:  0.9987991594115881\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9134396355353075\n",
      "- validation loss:  0.44286463205436005\n",
      "- validation roc_auc: 0.9612492611180009\n",
      "- validation precision:  0.8600583090379009\n",
      "- validation recall:  0.817174515235457\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.0212 - acc: 0.9929 - val_loss: 0.4429 - val_acc: 0.9134\n",
      "Epoch 175/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9942- train accuracy:  0.9990715732613099\n",
      "- train loss:  0.016870766241974276\n",
      "- train roc_auc: 0.9999880993313375\n",
      "- train precision:  0.9976004799040192\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9164768413059985\n",
      "- validation loss:  0.4485863361199335\n",
      "- validation roc_auc: 0.960056184007696\n",
      "- validation precision:  0.8575498575498576\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0168 - acc: 0.9940 - val_loss: 0.4486 - val_acc: 0.9165\n",
      "Epoch 176/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9927- train accuracy:  0.9966239027683997\n",
      "- train loss:  0.022641945179613127\n",
      "- train roc_auc: 0.9999927538150812\n",
      "- train precision:  0.9881270406648858\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9134396355353075\n",
      "- validation loss:  0.5273843380476185\n",
      "- validation roc_auc: 0.9615629237705583\n",
      "- validation precision:  0.8224543080939948\n",
      "- validation recall:  0.8725761772853186\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0225 - acc: 0.9928 - val_loss: 0.5274 - val_acc: 0.9134\n",
      "Epoch 177/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9943- train accuracy:  0.9991559756920999\n",
      "- train loss:  0.01823437396186236\n",
      "- train roc_auc: 0.9999928948600431\n",
      "- train precision:  0.9976011994002999\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.4774576021399386\n",
      "- validation roc_auc: 0.9658310539065127\n",
      "- validation precision:  0.8386243386243386\n",
      "- validation recall:  0.8781163434903048\n",
      "186/186 [==============================] - 3s 19ms/step - loss: 0.0181 - acc: 0.9944 - val_loss: 0.4775 - val_acc: 0.9203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9899- train accuracy:  0.9991559756920999\n",
      "- train loss:  0.02904118572083753\n",
      "- train roc_auc: 0.9999972672538626\n",
      "- train precision:  0.9973029667365898\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9195140470766895\n",
      "- validation loss:  0.45365497150833894\n",
      "- validation roc_auc: 0.9602930608838767\n",
      "- validation precision:  0.8493150684931506\n",
      "- validation recall:  0.8587257617728532\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0289 - acc: 0.9898 - val_loss: 0.4537 - val_acc: 0.9195\n",
      "Epoch 179/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9919- train accuracy:  0.99966239027684\n",
      "- train loss:  0.025162041440082918\n",
      "- train roc_auc: 0.9999995944957345\n",
      "- train precision:  0.999099369558691\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.44685033884750863\n",
      "- validation roc_auc: 0.9590174028442611\n",
      "- validation precision:  0.875\n",
      "- validation recall:  0.814404432132964\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0250 - acc: 0.9919 - val_loss: 0.4469 - val_acc: 0.9172\n",
      "Epoch 180/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9906- train accuracy:  0.99966239027684\n",
      "- train loss:  0.02734749078037498\n",
      "- train roc_auc: 0.9999991713608487\n",
      "- train precision:  0.999099369558691\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9271070615034168\n",
      "- validation loss:  0.45757735060667937\n",
      "- validation roc_auc: 0.9606385968775717\n",
      "- validation precision:  0.9076923076923077\n",
      "- validation recall:  0.817174515235457\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0272 - acc: 0.9907 - val_loss: 0.4576 - val_acc: 0.9271\n",
      "Epoch 181/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9915- train accuracy:  0.99966239027684\n",
      "- train loss:  0.02649457649409819\n",
      "- train roc_auc: 0.9999968441189769\n",
      "- train precision:  0.9996994289149383\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9179954441913439\n",
      "- validation loss:  0.5183679916321009\n",
      "- validation roc_auc: 0.9622605153049989\n",
      "- validation precision:  0.9067524115755627\n",
      "- validation recall:  0.7811634349030471\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0264 - acc: 0.9913 - val_loss: 0.5184 - val_acc: 0.9180\n",
      "Epoch 182/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9911- train accuracy:  0.99957798784605\n",
      "- train loss:  0.026608663718764792\n",
      "- train roc_auc: 0.9999978666949509\n",
      "- train precision:  0.9993990384615384\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9278663629460896\n",
      "- validation loss:  0.4467729170076215\n",
      "- validation roc_auc: 0.9595628716141819\n",
      "- validation precision:  0.8982035928143712\n",
      "- validation recall:  0.8310249307479224\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0265 - acc: 0.9912 - val_loss: 0.4468 - val_acc: 0.9279\n",
      "Epoch 183/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9895- train accuracy:  0.99949358541526\n",
      "- train loss:  0.03573165091128296\n",
      "- train roc_auc: 0.9999997002794561\n",
      "- train precision:  0.9982008995502248\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9179954441913439\n",
      "- validation loss:  0.4421741632050244\n",
      "- validation roc_auc: 0.9599612883784003\n",
      "- validation precision:  0.8645533141210374\n",
      "- validation recall:  0.8310249307479224\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0356 - acc: 0.9897 - val_loss: 0.4422 - val_acc: 0.9180\n",
      "Epoch 184/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9909- train accuracy:  0.99966239027684\n",
      "- train loss:  0.02552856244763094\n",
      "- train roc_auc: 0.999999347667051\n",
      "- train precision:  0.9987998799879988\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.4954673398056986\n",
      "- validation roc_auc: 0.9589652464678542\n",
      "- validation precision:  0.8706896551724138\n",
      "- validation recall:  0.8393351800554016\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0255 - acc: 0.9910 - val_loss: 0.4955 - val_acc: 0.9218\n",
      "Epoch 185/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0212 - acc: 0.9933- train accuracy:  0.99957798784605\n",
      "- train loss:  0.02142079705080825\n",
      "- train roc_auc: 0.9999991008383677\n",
      "- train precision:  0.9993990384615384\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.5266625516190163\n",
      "- validation roc_auc: 0.9615020746647505\n",
      "- validation precision:  0.9102564102564102\n",
      "- validation recall:  0.7867036011080333\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0213 - acc: 0.9933 - val_loss: 0.5267 - val_acc: 0.9203\n",
      "Epoch 186/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9924- train accuracy:  0.9992403781228899\n",
      "- train loss:  0.024553536585781154\n",
      "- train roc_auc: 0.9999989245321653\n",
      "- train precision:  0.9996989765201686\n",
      "- train recall:  0.9975968759387204\n",
      "- validation accuracy:  0.9172361427486713\n",
      "- validation loss:  0.5299895484518081\n",
      "- validation roc_auc: 0.9590891178618204\n",
      "- validation precision:  0.9064516129032258\n",
      "- validation recall:  0.778393351800554\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0244 - acc: 0.9924 - val_loss: 0.5300 - val_acc: 0.9172\n",
      "Epoch 187/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9924- train accuracy:  0.9993247805536799\n",
      "- train loss:  0.0240361766604955\n",
      "- train roc_auc: 0.9999932122112074\n",
      "- train precision:  0.9984989492644851\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.911921032649962\n",
      "- validation loss:  0.4693617704879541\n",
      "- validation roc_auc: 0.9557764635658734\n",
      "- validation precision:  0.855072463768116\n",
      "- validation recall:  0.817174515235457\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0239 - acc: 0.9925 - val_loss: 0.4694 - val_acc: 0.9119\n",
      "Epoch 188/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9928- train accuracy:  0.9992403781228899\n",
      "- train loss:  0.02262829262311344\n",
      "- train roc_auc: 0.9999968088577365\n",
      "- train precision:  0.9981992797118847\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9164768413059985\n",
      "- validation loss:  0.5074783535101419\n",
      "- validation roc_auc: 0.9589493097972854\n",
      "- validation precision:  0.8438356164383561\n",
      "- validation recall:  0.853185595567867\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0225 - acc: 0.9929 - val_loss: 0.5075 - val_acc: 0.9165\n",
      "Epoch 189/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9933- train accuracy:  0.99974679270763\n",
      "- train loss:  0.020429543204748483\n",
      "- train roc_auc: 0.9999997355406964\n",
      "- train precision:  0.9990996398559424\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9225512528473804\n",
      "- validation loss:  0.49756044119981174\n",
      "- validation roc_auc: 0.958884114326777\n",
      "- validation precision:  0.8912386706948641\n",
      "- validation recall:  0.817174515235457\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0203 - acc: 0.9933 - val_loss: 0.4976 - val_acc: 0.9226\n",
      "Epoch 190/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9917- train accuracy:  0.99974679270763\n",
      "- train loss:  0.02723356439133269\n",
      "- train roc_auc: 0.9999995239732535\n",
      "- train precision:  0.9990996398559424\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.4738341758625926\n",
      "- validation roc_auc: 0.9606451164246224\n",
      "- validation precision:  0.869942196531792\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0271 - acc: 0.9918 - val_loss: 0.4738 - val_acc: 0.9203\n",
      "Epoch 191/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0245 - acc: 0.9916- train accuracy:  0.9979743416610398\n",
      "- train loss:  0.02478192132841236\n",
      "- train roc_auc: 0.9999795308499007\n",
      "- train precision:  0.994612391499551\n",
      "- train recall:  0.9981976569540403\n",
      "- validation accuracy:  0.9126803340926348\n",
      "- validation loss:  0.485427489041739\n",
      "- validation roc_auc: 0.9557272047659338\n",
      "- validation precision:  0.8306451612903226\n",
      "- validation recall:  0.8559556786703602\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0247 - acc: 0.9915 - val_loss: 0.4854 - val_acc: 0.9127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9931- train accuracy:  0.99966239027684\n",
      "- train loss:  0.01948013559687812\n",
      "- train roc_auc: 0.9999989597934058\n",
      "- train precision:  0.999099369558691\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9271070615034168\n",
      "- validation loss:  0.4467952792266145\n",
      "- validation roc_auc: 0.9630740098981212\n",
      "- validation precision:  0.8885630498533724\n",
      "- validation recall:  0.8393351800554016\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0194 - acc: 0.9931 - val_loss: 0.4468 - val_acc: 0.9271\n",
      "Epoch 193/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9932- train accuracy:  0.99957798784605\n",
      "- train loss:  0.021468954071701758\n",
      "- train roc_auc: 0.9999958568042433\n",
      "- train precision:  0.9987995198079231\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9331814730447988\n",
      "- validation loss:  0.3891504651321637\n",
      "- validation roc_auc: 0.9647321480313866\n",
      "- validation precision:  0.9050445103857567\n",
      "- validation recall:  0.8448753462603878\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0214 - acc: 0.9933 - val_loss: 0.3892 - val_acc: 0.9332\n",
      "Epoch 194/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9933- train accuracy:  0.9989027683997299\n",
      "- train loss:  0.02166716260438765\n",
      "- train roc_auc: 0.9999934943011313\n",
      "- train precision:  0.9964071856287425\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9240698557327259\n",
      "- validation loss:  0.45867802283034326\n",
      "- validation roc_auc: 0.968054943844968\n",
      "- validation precision:  0.8635097493036211\n",
      "- validation recall:  0.8587257617728532\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0217 - acc: 0.9933 - val_loss: 0.4587 - val_acc: 0.9241\n",
      "Epoch 195/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9919- train accuracy:  0.9994091829844699\n",
      "- train loss:  0.028507472462595267\n",
      "- train roc_auc: 0.999999559234494\n",
      "- train precision:  0.9979016786570744\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9179954441913439\n",
      "- validation loss:  0.4571775913962652\n",
      "- validation roc_auc: 0.9624162600401025\n",
      "- validation precision:  0.839142091152815\n",
      "- validation recall:  0.8670360110803325\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0284 - acc: 0.9918 - val_loss: 0.4572 - val_acc: 0.9180\n",
      "Epoch 196/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9892- train accuracy:  0.99966239027684\n",
      "- train loss:  0.03275708017586889\n",
      "- train roc_auc: 0.9999988540096844\n",
      "- train precision:  0.9993992189846801\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9217919514047077\n",
      "- validation loss:  0.404287167317775\n",
      "- validation roc_auc: 0.9632196131155901\n",
      "- validation precision:  0.8603351955307262\n",
      "- validation recall:  0.853185595567867\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0326 - acc: 0.9889 - val_loss: 0.4043 - val_acc: 0.9218\n",
      "Epoch 197/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9923- train accuracy:  0.99974679270763\n",
      "- train loss:  0.02480835565097412\n",
      "- train roc_auc: 0.9999992418833297\n",
      "- train precision:  0.9996995192307693\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9233105542900532\n",
      "- validation loss:  0.4211265458785501\n",
      "- validation roc_auc: 0.9650523302309948\n",
      "- validation precision:  0.9037267080745341\n",
      "- validation recall:  0.8060941828254847\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0247 - acc: 0.9924 - val_loss: 0.4211 - val_acc: 0.9233\n",
      "Epoch 198/500\n",
      "185/186 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9914- train accuracy:  0.99957798784605\n",
      "- train loss:  0.02230154516525657\n",
      "- train roc_auc: 0.9999997355406964\n",
      "- train precision:  0.998500299940012\n",
      "- train recall:  1.0\n",
      "- validation accuracy:  0.9301442672741078\n",
      "- validation loss:  0.4437210601482232\n",
      "- validation roc_auc: 0.9664822842174805\n",
      "- validation precision:  0.8831908831908832\n",
      "- validation recall:  0.8587257617728532\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0222 - acc: 0.9914 - val_loss: 0.4437 - val_acc: 0.9301\n",
      "Epoch 199/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9940- train accuracy:  0.99966239027684\n",
      "- train loss:  0.01688862044290442\n",
      "- train roc_auc: 0.9999980430011534\n",
      "- train precision:  0.9993992189846801\n",
      "- train recall:  0.9993992189846801\n",
      "- validation accuracy:  0.9255884586180714\n",
      "- validation loss:  0.4544861518983689\n",
      "- validation roc_auc: 0.9644807832728707\n",
      "- validation precision:  0.8767908309455588\n",
      "- validation recall:  0.8476454293628809\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0168 - acc: 0.9940 - val_loss: 0.4545 - val_acc: 0.9256\n",
      "Epoch 200/500\n",
      "184/186 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9935- train accuracy:  0.99966239027684\n",
      "- train loss:  0.019932637943458488\n",
      "- train roc_auc: 0.9999980782623937\n",
      "- train precision:  0.9996994289149383\n",
      "- train recall:  0.9990988284770201\n",
      "- validation accuracy:  0.9301442672741078\n",
      "- validation loss:  0.44127842062064526\n",
      "- validation roc_auc: 0.9668444812758609\n",
      "- validation precision:  0.8853868194842407\n",
      "- validation recall:  0.8559556786703602\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0198 - acc: 0.9934 - val_loss: 0.4413 - val_acc: 0.9301\n",
      "Epoch 201/500\n",
      "183/186 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9932- train accuracy:  0.99974679270763\n",
      "- train loss:  0.021849248788676712\n",
      "- train roc_auc: 0.9999995944957345\n",
      "- train precision:  0.9993993993993994\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9202733485193622\n",
      "- validation loss:  0.4545263305430894\n",
      "- validation roc_auc: 0.96668366578194\n",
      "- validation precision:  0.8855421686746988\n",
      "- validation recall:  0.814404432132964\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0218 - acc: 0.9931 - val_loss: 0.4545 - val_acc: 0.9203\n",
      "Epoch 202/500\n",
      "182/186 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9939- train accuracy:  0.99966239027684\n",
      "- train loss:  0.017132843792575125\n",
      "- train roc_auc: 0.9999989950546464\n",
      "- train precision:  0.999099369558691\n",
      "- train recall:  0.99969960949234\n",
      "- validation accuracy:  0.9286256643887624\n",
      "- validation loss:  0.4147202482527644\n",
      "- validation roc_auc: 0.9685373903267307\n",
      "- validation precision:  0.8985074626865671\n",
      "- validation recall:  0.8337950138504155\n",
      "186/186 [==============================] - 4s 19ms/step - loss: 0.0171 - acc: 0.9940 - val_loss: 0.4147 - val_acc: 0.9286\n",
      "Epoch 00202: early stopping\n"
     ]
    }
   ],
   "source": [
    "X,y = prep.load_faces_dataset()\n",
    "X,y, class_weight = prep.prepare_data(X,y)\n",
    "model = emotion_recognition_cnn.build_model()\n",
    "trainer = trainer.Trainer(X, y, augment=False)\n",
    "model, metrics = trainer.train(model, \"dump/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': {'loss': [0.35400619480586359, 0.27705818545936001, 0.25691545521027226, 0.23291504364984, 0.23417351229134706, 0.24597409745464746, 0.20550812754833797, 0.23241566229065713, 0.20131529536135015, 0.19647743621375766, 0.20283077454693677, 0.19229500622530279, 0.23098051676989145, 0.19159634758689317, 0.19239000560844496, 0.18120567345582994, 0.204915923948259, 0.18382398129052616, 0.20754513524659401, 0.19290712049221034, 0.1898169164618671, 0.19553943389097486, 0.1943597458331261, 0.18998057971692212, 0.18995569905459925, 0.20052822897084843, 0.20369950332873321, 0.21487437936512013, 0.22184391861715005, 0.20560246116524858, 0.20461034933182898, 0.22798851546502602, 0.23457880031908654, 0.21660317944457519, 0.246140261377995, 0.22360851650176486, 0.26078184685565886, 0.27160940653642379, 0.25128463838438964, 0.26472506628583214, 0.28357174551296888, 0.25669407119285725, 0.27589030187059371, 0.26590256137248686, 0.27407317860341923, 0.30665187321563697, 0.32619461591441068, 0.31180710848435, 0.32191266957882597, 0.29070355044501067, 0.27416071139598491, 0.32049130162417933, 0.32958653345194927, 0.31147243217165577, 0.30086762656655019, 0.30884723518052243, 0.33529188667813664, 0.33331935439222043, 0.30662967150376075, 0.31130008717423241, 0.30614836988072552, 0.29611805205826697, 0.33775266206164928, 0.37691091337073634, 0.37538815757589261, 0.33930649378124111, 0.3556563137332508, 0.33053428251451855, 0.36017364933351359, 0.35364397502615313, 0.33397405457116475, 0.36677705743770844, 0.3300278029300629, 0.32789554015288502, 0.33121831609882363, 0.35107430289573716, 0.37190639807495457, 0.37850964739183268, 0.36376806023271135, 0.37163525584437401, 0.34732286092360631, 0.38866090063987219, 0.35208045445161962, 0.34346464018343792, 0.35728514253323784, 0.33511496762389381, 0.37341422109596639, 0.38530571422707249, 0.36256987694863035, 0.41807661309057598, 0.39583796783206854, 0.38608824808487135, 0.40279968068195998, 0.39976383228598894, 0.40149460906048301, 0.37463301770960461, 0.35560662468545984, 0.41549101469638045, 0.38890135340285104, 0.39095303133390308, 0.37723832358531906, 0.44730667241523009, 0.41582139477914448, 0.40593636859612836, 0.36883418317445771, 0.35115017217492733, 0.3939276119027974, 0.41443424715449073, 0.40589451596378467, 0.38974544469298245, 0.37530171414080588, 0.40149648373291724, 0.4363667921035147, 0.40742377888187825, 0.41214207183617757, 0.44407000080961662, 0.40951996783732281, 0.4874611146176685, 0.42454792073755981, 0.37187725620099604, 0.42628576258592021, 0.46127986152152173, 0.47511053809453435, 0.43839540962206203, 0.43548756254018034, 0.44129600099251864, 0.45676399458513717, 0.43205002708333679, 0.51255251144765446, 0.43502586502963697, 0.47873518932200598, 0.49832952624482463, 0.41376383119491167, 0.41471944114613368, 0.44803152296157522, 0.42981017011535283, 0.42158331353735001, 0.4795732005423638, 0.43343148877789056, 0.45015388138835505, 0.4937207819208016, 0.43516958427411095, 0.3933995895161263, 0.45188622874750184, 0.42445671508596444, 0.46084507152539994, 0.48376209874359516, 0.43510376891133995, 0.44883750310067255, 0.45050770600998863, 0.39930316952927691, 0.43066782510543106, 0.40425286842008024, 0.43131680149604806, 0.44924926868694337, 0.41189800077255873, 0.39486627594123036, 0.42314709531781158, 0.42299339109600359, 0.44865402952685174, 0.47890218571903315, 0.48371317844637074, 0.48618396924315027, 0.47400434902425781, 0.48380841155258564, 0.48960282760491947, 0.47487158465765605, 0.50552673270146598, 0.46493046520873282, 0.53142917147167901, 0.45192550155549854, 0.5115785463886362, 0.48976148075467268, 0.44286463205436005, 0.44858633611993348, 0.52738433804761853, 0.4774576021399386, 0.45365497150833894, 0.44685033884750863, 0.45757735060667937, 0.5183679916321009, 0.44677291700762151, 0.4421741632050244, 0.49546733980569863, 0.52666255161901632, 0.52998954845180812, 0.46936177048795408, 0.50747835351014192, 0.49756044119981174, 0.47383417586259258, 0.48542748904173899, 0.44679527922661449, 0.38915046513216373, 0.45867802283034326, 0.45717759139626518, 0.40428716731777498, 0.42112654587855008, 0.44372106014822321, 0.45448615189836888, 0.44127842062064526, 0.45452633054308939, 0.41472024825276438], 'roc_auc': [0.89768367737224586, 0.9397477949443086, 0.94742637258197249, 0.95463554283197527, 0.95668123181770759, 0.95419511120898481, 0.9636585959503472, 0.9638875044912435, 0.96499148112518696, 0.96661991909966516, 0.96518851632494584, 0.96709222406379314, 0.96647504027631292, 0.96820489342713745, 0.96769202239247099, 0.97125314387046668, 0.96593609105344291, 0.96951749556670808, 0.96160711181168068, 0.96799626792151039, 0.9684193140856987, 0.96791803335690019, 0.96864242747366103, 0.96872355961473833, 0.97094889834142717, 0.97066203827118991, 0.96750875068093045, 0.96422362336142042, 0.9627255763279593, 0.9697753798722748, 0.97162403365824823, 0.96437936809652403, 0.96648663058218109, 0.96515664298380832, 0.96253723385760148, 0.96524936543075379, 0.96254882416346965, 0.95999678369012154, 0.96625482446481759, 0.96401137588520958, 0.96136371538844911, 0.96376218430904392, 0.96362020306215879, 0.96390996070886314, 0.96728201532238434, 0.96259446099282564, 0.95809959549832524, 0.95540992014279258, 0.95774102041052867, 0.95901667845014438, 0.96810275385667421, 0.96147816965889721, 0.96192656961717216, 0.96230977410493868, 0.96160276544698009, 0.96124274157095013, 0.9613687861472664, 0.95806482458072073, 0.96388098494419272, 0.96163101681753382, 0.96185485459961284, 0.96028654133682578, 0.963813616291334, 0.95967587709639668, 0.96146513056479566, 0.96162667045283312, 0.95967297951992969, 0.96358760532690457, 0.96172591244682959, 0.96119130958865995, 0.96640767162345409, 0.96304575852756757, 0.96561808203618482, 0.96056977943647937, 0.96083925404791437, 0.9636564227679969, 0.9606682970363587, 0.96378319173842997, 0.96254302901053568, 0.96277773270436606, 0.96285306969250917, 0.96347025347998927, 0.96565864810672353, 0.96404687119693089, 0.96582453435946181, 0.9641606010732624, 0.96249521899882939, 0.96297838987470885, 0.9662193291530965, 0.95847772922727426, 0.96403238331459562, 0.96567458477729229, 0.96135067629434745, 0.95745271155205791, 0.96282119635137176, 0.95934120701445313, 0.96566661644200791, 0.96259373659870895, 0.96319933008032099, 0.96331161116841879, 0.96601070364746933, 0.96138182524136817, 0.9655521621715597, 0.96528920710717558, 0.96248000672237755, 0.96548479351870098, 0.96133184204731159, 0.96589335180055391, 0.96405846150279917, 0.96353472455638101, 0.9654355347187612, 0.95974541893160559, 0.96198162357004602, 0.96379767962076524, 0.96042924697782772, 0.95761859780479608, 0.9639273461676654, 0.9581915935511538, 0.96413524727917577, 0.9603597051426187, 0.96500596900752211, 0.95374091609777578, 0.95812060292771128, 0.9603988224249238, 0.96475967500782334, 0.95806120261013683, 0.96205840934642262, 0.96363469094449405, 0.96373538172672379, 0.96257490235167309, 0.96236772563427952, 0.96244958216947341, 0.96247421156944335, 0.95957880828475062, 0.95685363761749676, 0.96225399575794812, 0.96721030030482513, 0.96059078686586541, 0.96379043567959755, 0.96375349157964285, 0.96137675448255078, 0.96585858088294951, 0.96201277251706674, 0.96019671646634763, 0.96266110525156756, 0.96162956802930033, 0.96423086730258811, 0.96505884977804568, 0.96435546309067099, 0.9628813210630629, 0.96565140416555595, 0.96440979264942805, 0.96269660056328887, 0.95790183590444955, 0.95920646970873558, 0.96719653681660656, 0.95853640515073191, 0.96037274423672037, 0.95930281412626472, 0.95666891711772273, 0.96092618134192564, 0.95833140161568864, 0.95409731800322217, 0.96216706846393674, 0.95744981397559081, 0.95838066041562842, 0.96060672353643417, 0.95803729760428369, 0.96102469894180498, 0.95498252761390368, 0.95901233208544379, 0.96486616094298716, 0.96073204371863374, 0.96124926111800091, 0.96005618400769599, 0.9615629237705583, 0.96583105390651269, 0.96029306088387667, 0.95901740284426107, 0.96063859687757169, 0.9622605153049989, 0.95956287161418186, 0.95996128837840033, 0.95896524646785419, 0.96150207466475046, 0.95908911786182038, 0.95577646356587342, 0.95894930979728543, 0.958884114326777, 0.96064511642462236, 0.95572720476593376, 0.9630740098981212, 0.96473214803138663, 0.96805494384496804, 0.9624162600401025, 0.96321961311559012, 0.96505233023099479, 0.9664822842174805, 0.96448078327287068, 0.96684448127586087, 0.96668366578193998, 0.9685373903267307], 'accuracy': [0.84965831435079731, 0.88762338648443428, 0.89825360668185272, 0.90888382687927105, 0.90508731966590739, 0.90888382687927105, 0.92027334851936216, 0.90508731966590739, 0.92862566438876237, 0.92255125284738038, 0.92255125284738038, 0.9248291571753986, 0.90964312832194383, 0.92558845861807137, 0.92255125284738038, 0.92862566438876237, 0.91647684130599849, 0.92179195140470771, 0.92027334851936216, 0.92406985573272593, 0.92406985573272593, 0.92406985573272593, 0.92331055429005315, 0.92634776006074415, 0.9248291571753986, 0.92255125284738038, 0.92331055429005315, 0.92103264996203493, 0.90888382687927105, 0.93090356871678059, 0.93166287015945326, 0.92634776006074415, 0.92331055429005315, 0.91647684130599849, 0.92255125284738038, 0.92027334851936216, 0.91116173120728927, 0.91268033409263483, 0.9278663629460896, 0.92103264996203493, 0.91723614274867127, 0.92179195140470771, 0.91951404707668949, 0.92255125284738038, 0.9248291571753986, 0.92406985573272593, 0.91419893697798027, 0.91495823842065305, 0.90964312832194383, 0.92255125284738038, 0.92710706150341682, 0.91875474563401671, 0.91951404707668949, 0.91495823842065305, 0.91723614274867127, 0.91951404707668949, 0.92027334851936216, 0.90964312832194383, 0.92179195140470771, 0.92179195140470771, 0.92027334851936216, 0.91723614274867127, 0.92710706150341682, 0.92103264996203493, 0.91495823842065305, 0.90964312832194383, 0.91723614274867127, 0.91495823842065305, 0.91723614274867127, 0.92103264996203493, 0.93242217160212604, 0.92634776006074415, 0.9248291571753986, 0.91419893697798027, 0.91495823842065305, 0.92179195140470771, 0.91951404707668949, 0.91723614274867127, 0.91495823842065305, 0.92179195140470771, 0.91571753986332571, 0.92255125284738038, 0.92710706150341682, 0.92331055429005315, 0.92103264996203493, 0.91723614274867127, 0.92255125284738038, 0.91495823842065305, 0.91875474563401671, 0.91419893697798027, 0.91951404707668949, 0.92331055429005315, 0.91495823842065305, 0.91571753986332571, 0.91951404707668949, 0.92103264996203493, 0.9248291571753986, 0.92027334851936216, 0.92406985573272593, 0.92027334851936216, 0.91951404707668949, 0.91875474563401671, 0.91192103264996205, 0.92179195140470771, 0.91875474563401671, 0.92027334851936216, 0.91343963553530749, 0.91951404707668949, 0.91799544419134393, 0.92027334851936216, 0.91875474563401671, 0.90736522399392561, 0.91875474563401671, 0.92179195140470771, 0.91268033409263483, 0.91799544419134393, 0.91875474563401671, 0.91419893697798027, 0.91723614274867127, 0.91647684130599849, 0.91799544419134393, 0.90964312832194383, 0.91343963553530749, 0.91875474563401671, 0.91875474563401671, 0.91268033409263483, 0.91192103264996205, 0.91343963553530749, 0.91343963553530749, 0.91723614274867127, 0.91495823842065305, 0.91268033409263483, 0.91951404707668949, 0.91495823842065305, 0.91495823842065305, 0.91875474563401671, 0.92255125284738038, 0.91419893697798027, 0.93242217160212604, 0.9248291571753986, 0.91875474563401671, 0.92406985573272593, 0.92027334851936216, 0.91343963553530749, 0.92331055429005315, 0.91799544419134393, 0.91875474563401671, 0.92179195140470771, 0.91723614274867127, 0.9278663629460896, 0.91875474563401671, 0.91951404707668949, 0.91799544419134393, 0.91268033409263483, 0.91951404707668949, 0.93014426727410782, 0.91799544419134393, 0.92027334851936216, 0.91268033409263483, 0.92027334851936216, 0.91875474563401671, 0.91723614274867127, 0.91647684130599849, 0.91495823842065305, 0.92331055429005315, 0.92255125284738038, 0.92331055429005315, 0.91343963553530749, 0.91875474563401671, 0.91495823842065305, 0.91571753986332571, 0.91495823842065305, 0.91571753986332571, 0.91343963553530749, 0.91647684130599849, 0.91343963553530749, 0.92027334851936216, 0.91951404707668949, 0.91723614274867127, 0.92710706150341682, 0.91799544419134393, 0.9278663629460896, 0.91799544419134393, 0.92179195140470771, 0.92027334851936216, 0.91723614274867127, 0.91192103264996205, 0.91647684130599849, 0.92255125284738038, 0.92027334851936216, 0.91268033409263483, 0.92710706150341682, 0.93318147304479881, 0.92406985573272593, 0.91799544419134393, 0.92179195140470771, 0.92331055429005315, 0.93014426727410782, 0.92558845861807137, 0.93014426727410782, 0.92027334851936216, 0.92862566438876237], 'recall': [0.60664819944598336, 0.79778393351800558, 0.79778393351800558, 0.80886426592797789, 0.72576177285318555, 0.83933518005540164, 0.81163434903047094, 0.87811634349030476, 0.81440443213296398, 0.80332409972299168, 0.85041551246537395, 0.81163434903047094, 0.7063711911357341, 0.86149584487534625, 0.8476454293628809, 0.83102493074792239, 0.8642659279778393, 0.84487534626038785, 0.80609418282548473, 0.84210526315789469, 0.84487534626038785, 0.8642659279778393, 0.80332409972299168, 0.82271468144044324, 0.8753462603878116, 0.82548476454293629, 0.81163434903047094, 0.82548476454293629, 0.76177285318559562, 0.82271468144044324, 0.86980609418282551, 0.81440443213296398, 0.81994459833795019, 0.81994459833795019, 0.85318559556786699, 0.83656509695290859, 0.87811634349030476, 0.78670360110803328, 0.85318559556786699, 0.8476454293628809, 0.84210526315789469, 0.82271468144044324, 0.84210526315789469, 0.80332409972299168, 0.83379501385041555, 0.8587257617728532, 0.83933518005540164, 0.79224376731301938, 0.78947368421052633, 0.81994459833795019, 0.85041551246537395, 0.83933518005540164, 0.78947368421052633, 0.82548476454293629, 0.85041551246537395, 0.80332409972299168, 0.78670360110803328, 0.81163434903047094, 0.85041551246537395, 0.80886426592797789, 0.82548476454293629, 0.83379501385041555, 0.8642659279778393, 0.79501385041551242, 0.8476454293628809, 0.88642659279778391, 0.77839335180055402, 0.79778393351800558, 0.83933518005540164, 0.85041551246537395, 0.8587257617728532, 0.79778393351800558, 0.81163434903047094, 0.83102493074792239, 0.85041551246537395, 0.83933518005540164, 0.83102493074792239, 0.8587257617728532, 0.84487534626038785, 0.78116343490304707, 0.83379501385041555, 0.80886426592797789, 0.81717451523545703, 0.83102493074792239, 0.84487534626038785, 0.86149584487534625, 0.83656509695290859, 0.87257617728531855, 0.85595567867036015, 0.85041551246537395, 0.84487534626038785, 0.85041551246537395, 0.83379501385041555, 0.79224376731301938, 0.86703601108033246, 0.8587257617728532, 0.86149584487534625, 0.81717451523545703, 0.79501385041551242, 0.80055401662049863, 0.81440443213296398, 0.83379501385041555, 0.8476454293628809, 0.83102493074792239, 0.84210526315789469, 0.84210526315789469, 0.82548476454293629, 0.86980609418282551, 0.78393351800554012, 0.83656509695290859, 0.82271468144044324, 0.79224376731301938, 0.8476454293628809, 0.82825484764542934, 0.79501385041551242, 0.79224376731301938, 0.80886426592797789, 0.78116343490304707, 0.79501385041551242, 0.83656509695290859, 0.82271468144044324, 0.79224376731301938, 0.77839335180055402, 0.83379501385041555, 0.8642659279778393, 0.78947368421052633, 0.86149584487534625, 0.82548476454293629, 0.85595567867036015, 0.85595567867036015, 0.84487534626038785, 0.81717451523545703, 0.83656509695290859, 0.80055401662049863, 0.80609418282548473, 0.81163434903047094, 0.85318559556786699, 0.81440443213296398, 0.83656509695290859, 0.81440443213296398, 0.77562326869806097, 0.81994459833795019, 0.83379501385041555, 0.80332409972299168, 0.8476454293628809, 0.82271468144044324, 0.84210526315789469, 0.84210526315789469, 0.82548476454293629, 0.8476454293628809, 0.86149584487534625, 0.8587257617728532, 0.83379501385041555, 0.80609418282548473, 0.81994459833795019, 0.83933518005540164, 0.84487534626038785, 0.84487534626038785, 0.85041551246537395, 0.82825484764542934, 0.8587257617728532, 0.80332409972299168, 0.83656509695290859, 0.82825484764542934, 0.82271468144044324, 0.80055401662049863, 0.83656509695290859, 0.83102493074792239, 0.82271468144044324, 0.77562326869806097, 0.80055401662049863, 0.8642659279778393, 0.79501385041551242, 0.81717451523545703, 0.83379501385041555, 0.87257617728531855, 0.87811634349030476, 0.8587257617728532, 0.81440443213296398, 0.81717451523545703, 0.78116343490304707, 0.83102493074792239, 0.83102493074792239, 0.83933518005540164, 0.78670360110803328, 0.77839335180055402, 0.81717451523545703, 0.85318559556786699, 0.81717451523545703, 0.83379501385041555, 0.85595567867036015, 0.83933518005540164, 0.84487534626038785, 0.8587257617728532, 0.86703601108033246, 0.85318559556786699, 0.80609418282548473, 0.8587257617728532, 0.8476454293628809, 0.85595567867036015, 0.81440443213296398, 0.83379501385041555], 'precision': [0.79636363636363638, 0.79338842975206614, 0.82521489971346706, 0.85131195335276966, 0.90972222222222221, 0.83013698630136989, 0.88787878787878793, 0.79648241206030146, 0.91588785046728971, 0.90342679127725856, 0.86478873239436616, 0.90432098765432101, 0.95149253731343286, 0.86629526462395545, 0.86685552407932009, 0.90090090090090091, 0.83646112600536193, 0.86647727272727271, 0.8926380368098159, 0.87608069164265134, 0.87392550143266479, 0.85950413223140498, 0.90625, 0.90000000000000002, 0.8540540540540541, 0.88427299703264095, 0.89877300613496935, 0.87905604719764008, 0.88996763754045305, 0.91666666666666663, 0.8795518207282913, 0.90740740740740744, 0.89156626506024095, 0.86803519061583578, 0.86274509803921573, 0.86781609195402298, 0.81282051282051282, 0.88198757763975155, 0.88, 0.86197183098591545, 0.8539325842696629, 0.8839285714285714, 0.86118980169971671, 0.90342679127725856, 0.88529411764705879, 0.86350974930362112, 0.84636871508379885, 0.88544891640866874, 0.86890243902439024, 0.88888888888888884, 0.87965616045845274, 0.86079545454545459, 0.90476190476190477, 0.85878962536023051, 0.84806629834254144, 0.89230769230769236, 0.91025641025641024, 0.85174418604651159, 0.86235955056179781, 0.89570552147239269, 0.87647058823529411, 0.85999999999999999, 0.86908077994428967, 0.90536277602523663, 0.84297520661157022, 0.8040201005025126, 0.90645161290322585, 0.88073394495412849, 0.85593220338983056, 0.85994397759103647, 0.89080459770114939, 0.92307692307692313, 0.90432098765432101, 0.85227272727272729, 0.84109589041095889, 0.87068965517241381, 0.86956521739130432, 0.84239130434782605, 0.84487534626038785, 0.92156862745098034, 0.85511363636363635, 0.89846153846153842, 0.90769230769230769, 0.88235294117647056, 0.86402266288951846, 0.8405405405405405, 0.87536231884057969, 0.82677165354330706, 0.84890109890109888, 0.83879781420765032, 0.85915492957746475, 0.86723163841807904, 0.85269121813031157, 0.88819875776397517, 0.84366576819407013, 0.85399449035812669, 0.86388888888888893, 0.88323353293413176, 0.91693290734824284, 0.89751552795031053, 0.88288288288288286, 0.86494252873563215, 0.83378746594005448, 0.8771929824561403, 0.85875706214689262, 0.86363636363636365, 0.85386819484240684, 0.8418230563002681, 0.90415335463258784, 0.86781609195402298, 0.87352941176470589, 0.85885885885885882, 0.85474860335195535, 0.87941176470588234, 0.875, 0.89655172413793105, 0.88484848484848488, 0.89240506329113922, 0.89130434782608692, 0.85552407932011332, 0.87096774193548387, 0.8666666666666667, 0.89206349206349211, 0.86494252873563215, 0.84324324324324329, 0.87962962962962965, 0.82493368700265257, 0.85386819484240684, 0.8328840970350404, 0.84426229508196726, 0.84487534626038785, 0.85755813953488369, 0.86532951289398286, 0.87841945288753798, 0.87387387387387383, 0.88253012048192769, 0.86274509803921573, 0.86470588235294121, 0.90963855421686746, 0.90184049079754602, 0.91503267973856206, 0.89425981873111782, 0.86994219653179194, 0.87087087087087089, 0.86931818181818177, 0.87096774193548387, 0.85875706214689262, 0.86857142857142855, 0.86627906976744184, 0.88439306358381498, 0.84510869565217395, 0.84931506849315064, 0.86246418338108888, 0.8660714285714286, 0.87833827893175076, 0.89910979228486643, 0.85434173669467783, 0.8615819209039548, 0.83423913043478259, 0.8742690058479532, 0.84699453551912574, 0.88414634146341464, 0.85552407932011332, 0.85673352435530081, 0.8892215568862275, 0.90595611285266453, 0.87790697674418605, 0.84985835694050993, 0.87352941176470589, 0.90032154340836013, 0.88109756097560976, 0.83199999999999996, 0.88580246913580252, 0.86005830903790093, 0.85754985754985757, 0.82245430809399478, 0.83862433862433861, 0.84931506849315064, 0.875, 0.90769230769230769, 0.90675241157556274, 0.89820359281437123, 0.86455331412103742, 0.87068965517241381, 0.91025641025641024, 0.90645161290322585, 0.85507246376811596, 0.84383561643835614, 0.89123867069486407, 0.86994219653179194, 0.83064516129032262, 0.88856304985337242, 0.90504451038575673, 0.86350974930362112, 0.83914209115281502, 0.86033519553072624, 0.90372670807453415, 0.88319088319088324, 0.87679083094555876, 0.88538681948424069, 0.88554216867469882, 0.89850746268656712]}, 'train': {'loss': [0.54438106008351939, 0.33620348784023646, 0.28811375449671284, 0.2688827521490294, 0.24560896256419149, 0.23796125646154878, 0.22356287479541498, 0.21610795588287288, 0.21258624279362218, 0.20600955393530082, 0.20155085410965529, 0.19502855144104067, 0.18882013231536007, 0.18644028939533122, 0.1749959311659959, 0.16893505744238627, 0.16516710342908533, 0.16284105677728633, 0.15960419685111346, 0.15598475294166284, 0.14974851949815812, 0.14026266300219448, 0.14100395060401563, 0.13164388939454036, 0.122702665259756, 0.11719894072872251, 0.11924796696352363, 0.11122516230513405, 0.11235570028590279, 0.1059373540540932, 0.095819193138440686, 0.093365378963341586, 0.09705416213753737, 0.091608912952541102, 0.081286015217918783, 0.077873511876436777, 0.080534057121957825, 0.075786622322260688, 0.072349674022740951, 0.065382739232197798, 0.063323909799133826, 0.059927302163036426, 0.061017103805279034, 0.072895281811689375, 0.057845696696230561, 0.052428253940800251, 0.05625369544483716, 0.057652032620334526, 0.05503483471170742, 0.053896040167569284, 0.050375758753592688, 0.054372434228895122, 0.052255951685630977, 0.048401914460548048, 0.052228036238053561, 0.04606876447910014, 0.044160282503119257, 0.042759776380201418, 0.046704444826745968, 0.048138621530466025, 0.051179023870819733, 0.054229756913887234, 0.040402461465883305, 0.036884297644134652, 0.034798588833893015, 0.042175769333329595, 0.038654801779747977, 0.045424772540464557, 0.02911733164759208, 0.034532324989408697, 0.03852220386289347, 0.043758830045532889, 0.037360481855445049, 0.049023473341644294, 0.044073473458126226, 0.035776898131647965, 0.02726877171729844, 0.03417849661327256, 0.041631801499105399, 0.03223187428132545, 0.033394008124364161, 0.039312968664263141, 0.03764136878466693, 0.041161435783905485, 0.040351580581605775, 0.030761192477873234, 0.031960288101051365, 0.030741180733732561, 0.034836534239102442, 0.034668139050916529, 0.042268650714121885, 0.023230605281117214, 0.028926558943199978, 0.039878504712582238, 0.029383099276024113, 0.022812033241155955, 0.038236546905234627, 0.037830350849026619, 0.026990559755445311, 0.031412445562140193, 0.034386291999769489, 0.031350866637856675, 0.027589867279973639, 0.029587706058710955, 0.039645750532436116, 0.038937062520523112, 0.03543810080170913, 0.023372794624128279, 0.027312631300503307, 0.029091167482081452, 0.032613381513901897, 0.034311528360313287, 0.036013264775230279, 0.035193646192048525, 0.028832824404944613, 0.023570966397675922, 0.022686498403519873, 0.032381626043957959, 0.030474832307174425, 0.025606253347765402, 0.020031054577710344, 0.025160407433800391, 0.028602209169503144, 0.021007744054992953, 0.023130190337581256, 0.050388734210420907, 0.035082998820583688, 0.032977609090874278, 0.022380949398924033, 0.027924384112559656, 0.020315904317655863, 0.029166042560568126, 0.03137679988280024, 0.021452296890757328, 0.02945344286227497, 0.028051723529736259, 0.022913804601283894, 0.028706417905036575, 0.021017632248781715, 0.02927668998672301, 0.034707374321479176, 0.031325695148878861, 0.024316742668490526, 0.021993961376779916, 0.026228580462890092, 0.020058187022619703, 0.02013130774291242, 0.028210151875105477, 0.022821315845103503, 0.025703184636098136, 0.031147291827740953, 0.024284203127193201, 0.033740235367132089, 0.025532586479689711, 0.032317915841261681, 0.027670252259078973, 0.026079315612479986, 0.023618535630847579, 0.021024763100180154, 0.029745104954605337, 0.03036126884344121, 0.03018355498650532, 0.028017612794707519, 0.031582550671205342, 0.037454720041799505, 0.018618341757470062, 0.025784439383530398, 0.020066627914362364, 0.020726083729168208, 0.024287496324770712, 0.022299420206764051, 0.017701437559725867, 0.023314204970802319, 0.021250480204635828, 0.016870766241974276, 0.022641945179613127, 0.018234373961862359, 0.029041185720837529, 0.025162041440082918, 0.027347490780374979, 0.026494576494098188, 0.026608663718764792, 0.035731650911282963, 0.02552856244763094, 0.021420797050808248, 0.024553536585781154, 0.024036176660495501, 0.022628292623113441, 0.020429543204748483, 0.02723356439133269, 0.024781921328412362, 0.019480135596878119, 0.021468954071701758, 0.021667162604387651, 0.028507472462595267, 0.03275708017586889, 0.024808355650974119, 0.022301545165256571, 0.016888620442904421, 0.019932637943458488, 0.021849248788676712, 0.017132843792575125], 'roc_auc': [0.90867706313782515, 0.94427491623604176, 0.94886329396897739, 0.96011720977380932, 0.96257297005181752, 0.96457748342007643, 0.9732615247573928, 0.97295873648538034, 0.97453801692405562, 0.97853353860546932, 0.97854751968732023, 0.97997083084403669, 0.97757291663103807, 0.98217073556111267, 0.98393774684411017, 0.98658808746240401, 0.98613466317105525, 0.98913934399494541, 0.98678069317322281, 0.9905228981030193, 0.99123082921285177, 0.99299762892840637, 0.99409185574302117, 0.99521197312345933, 0.9948079762759553, 0.99643076908538442, 0.99658383813031359, 0.99563354769934331, 0.99465649398684775, 0.998680542011811, 0.99868061253429197, 0.9987420376152103, 0.99910395898751014, 0.99931912307692683, 0.99910438212239594, 0.99946210740707842, 0.99920642815234872, 0.99923407296488609, 0.99987293611992567, 0.99972230010058971, 0.99991563748214851, 0.99977247684579451, 0.99988002362926243, 0.99982378195069477, 0.99990788000924269, 0.99996391012036745, 0.99973742717275615, 0.99986535495322237, 0.99994575058151947, 0.99995650525986635, 0.99993982669311876, 0.99995840936685232, 0.99987004469820628, 0.99997568737468812, 0.9999288251860885, 0.99997480584367615, 0.99995816253816905, 0.99996020769011695, 0.99998852246622327, 0.99993619478534912, 0.999789578547428, 0.9999286136186456, 0.99996436851649362, 0.99997237281808293, 0.99998295119022729, 0.99996898773899678, 0.99997628681577633, 0.99998432637860613, 0.99999243646391678, 0.99999226015771436, 0.99998940399723535, 0.99984850008027215, 0.99997896667005293, 0.99995371962186841, 0.99999039131196876, 0.99999638572285066, 0.9999971262089008, 0.99999508105695289, 0.99999451687710517, 0.99999765512750793, 0.99995241495597043, 0.9999771683467884, 0.99999398795849792, 0.99998443216232746, 0.99998809933133759, 0.99997773252663613, 0.99997826144524327, 0.99999592732672449, 0.99999430530966227, 0.9999029081743348, 0.99999952397325353, 0.99999821930735566, 0.99999391743601707, 0.99998693571040165, 0.99999850139727964, 0.99992854309616463, 0.99999314168872633, 0.99999335325616934, 0.99999349430113116, 0.99992713264654542, 0.99998319801891067, 0.99999850139727964, 0.999979848201065, 0.99999825456859615, 0.99995548268389234, 0.99999014448328549, 0.99999850139727964, 0.99999871296472242, 0.99999656202905307, 0.99999356482361212, 0.99999761986626745, 0.99993221026517476, 0.99998711201660406, 0.99999698516393876, 0.9999954689305981, 0.99996814146922519, 0.99999271855384064, 0.99999828982983652, 0.99999698516393876, 0.99998725306156599, 0.9999993829282916, 0.99995230917224909, 0.99999303590500488, 0.99999638572285066, 0.99999807826239384, 0.99999352956237164, 0.99999635046161006, 0.99999254224763812, 0.99999769038874853, 0.99998915716855208, 0.99999539840811724, 0.99999328273368837, 0.99999247172515715, 0.99999141388794288, 0.99999151967166422, 0.99999476370578855, 0.99999483422826951, 0.99999042657320936, 0.99999867770348194, 0.99999599784920534, 0.99999120232049998, 0.99999920662208919, 0.99999892453216521, 0.99999529262439579, 0.99999934766705112, 0.99999740829882466, 0.99999786669495094, 0.99999790195619143, 0.99999962975697487, 0.99999419952594093, 0.99999627993912921, 0.99999952397325353, 0.99999888927092484, 0.99999427004842179, 0.99999814878487481, 0.99999783143371035, 0.99998192861425339, 0.99999962975697487, 0.99999663255153404, 0.99999374112981454, 0.99999931240581064, 0.9999926127701193, 0.99993503116441329, 0.99999578628176256, 0.99999867770348194, 0.99999910083836774, 0.99999818404611518, 0.99999843087479867, 0.99999966501821547, 0.99999659729029355, 0.99995234443348957, 0.9999991713608487, 0.99999970027945584, 0.99999761986626756, 0.99998809933133748, 0.99999275381508124, 0.99999289486004306, 0.99999726725386262, 0.9999995944957345, 0.9999991713608487, 0.99999684411897694, 0.99999786669495094, 0.99999970027945606, 0.99999934766705101, 0.99999910083836774, 0.99999892453216532, 0.99999321221120741, 0.99999680885773645, 0.99999973554069643, 0.99999952397325353, 0.99997953084990066, 0.99999895979340581, 0.9999958568042433, 0.99999349430113127, 0.99999955923449402, 0.99999885400968436, 0.99999924188332967, 0.99999973554069643, 0.99999804300115336, 0.99999807826239373, 0.9999995944957345, 0.9999989950546464], 'accuracy': [0.85761309925725859, 0.89846387575962183, 0.90268399729912219, 0.91677920324105333, 0.91677920324105333, 0.91483794733288315, 0.93222484807562456, 0.91787643484132342, 0.93095881161377447, 0.937711006076975, 0.93585415259959481, 0.94032748143146527, 0.92656988521269412, 0.94134031060094536, 0.94429439567859552, 0.95138419986495615, 0.94724848075624579, 0.95729237002025658, 0.95138419986495615, 0.95796758946657667, 0.96007765023632685, 0.96210330857528692, 0.96801147873058746, 0.9734132343011479, 0.96910871033085755, 0.97721134368669815, 0.97594530722484807, 0.97164078325455772, 0.96657663740715738, 0.98539837947332887, 0.98480756245779877, 0.98514517218095876, 0.98725523295070894, 0.98987170830519922, 0.98784604996623904, 0.9912221471978393, 0.98016542876434842, 0.98784604996623904, 0.99577987846049965, 0.99409182984469957, 0.99687711006076973, 0.99324780553679948, 0.99653950033760974, 0.99468264686022956, 0.99805874409182982, 0.99772113436866983, 0.99257258609047938, 0.99561107359891965, 0.99805874409182982, 0.99788993923024982, 0.99738352464550983, 0.99805874409182982, 0.99459824442943956, 0.99907157326130991, 0.99637069547602974, 0.99746792707629983, 0.99780553679945982, 0.99805874409182982, 0.99822754895340982, 0.99797434166103982, 0.99502025658338955, 0.99890276839972991, 0.99924037812288991, 0.99772113436866983, 0.99814314652261982, 0.99442943956785956, 0.99814314652261982, 0.99881836596893991, 0.99907157326130991, 0.99898717083051991, 0.99949358541526001, 0.99451384199864956, 0.99797434166103982, 0.99729912221471984, 0.99848075624577992, 0.9994091829844699, 0.99907157326130991, 0.9993247805536799, 0.99890276839972991, 0.99949358541526001, 0.99729912221471984, 0.99831195138419981, 0.99907157326130991, 0.99881836596893991, 0.99898717083051991, 0.99814314652261982, 0.99822754895340982, 0.99797434166103982, 0.99873396353814992, 0.99569547602970965, 0.99966239027684001, 0.99966239027684001, 0.99873396353814992, 0.99915597569209991, 0.99881836596893991, 0.99729912221471984, 0.99957798784605001, 0.9993247805536799, 0.99949358541526001, 0.99645509790681974, 0.99907157326130991, 0.99974679270763001, 0.99890276839972991, 0.99957798784605001, 0.99738352464550983, 0.99924037812288991, 0.99957798784605001, 0.9993247805536799, 0.99924037812288991, 0.99957798784605001, 0.99957798784605001, 0.99687711006076973, 0.99957798784605001, 0.99957798784605001, 0.99966239027684001, 0.99788993923024982, 0.9993247805536799, 0.9993247805536799, 0.9994091829844699, 0.99949358541526001, 0.99966239027684001, 0.99729912221471984, 0.99898717083051991, 0.99949358541526001, 0.9993247805536799, 0.9994091829844699, 0.99881836596893991, 0.99907157326130991, 0.99864956110735992, 0.99949358541526001, 0.9994091829844699, 0.99974679270763001, 0.9993247805536799, 0.99924037812288991, 0.99848075624577992, 0.99966239027684001, 0.99974679270763001, 0.9993247805536799, 0.99966239027684001, 0.9993247805536799, 0.99831195138419981, 0.99957798784605001, 0.99949358541526001, 0.99949358541526001, 0.99957798784605001, 0.99949358541526001, 0.9993247805536799, 0.99966239027684001, 0.99957798784605001, 0.99907157326130991, 0.9993247805536799, 0.99907157326130991, 0.99957798784605001, 0.99890276839972991, 0.99957798784605001, 0.99924037812288991, 0.99704591492234973, 0.99966239027684001, 0.99898717083051991, 0.9994091829844699, 0.99966239027684001, 0.99898717083051991, 0.99662390276839974, 0.99966239027684001, 0.99966239027684001, 0.99974679270763001, 0.99949358541526001, 0.99907157326130991, 0.99966239027684001, 0.99957798784605001, 0.99822754895340982, 0.99949358541526001, 0.99966239027684001, 0.99949358541526001, 0.99907157326130991, 0.99662390276839974, 0.99915597569209991, 0.99915597569209991, 0.99966239027684001, 0.99966239027684001, 0.99966239027684001, 0.99957798784605001, 0.99949358541526001, 0.99966239027684001, 0.99957798784605001, 0.99924037812288991, 0.9993247805536799, 0.99924037812288991, 0.99974679270763001, 0.99974679270763001, 0.99797434166103982, 0.99966239027684001, 0.99957798784605001, 0.99890276839972991, 0.9994091829844699, 0.99966239027684001, 0.99974679270763001, 0.99957798784605001, 0.99966239027684001, 0.99966239027684001, 0.99974679270763001, 0.99966239027684001], 'recall': [0.62901772303995196, 0.8257735055572244, 0.83478522078702311, 0.8419945929708621, 0.77831180534695099, 0.87533793932111748, 0.86662661459897872, 0.91649143887053164, 0.85100630820066081, 0.85971763292279968, 0.89606488434965459, 0.87683989185941724, 0.77801141483929104, 0.90988284770201266, 0.90327425653349358, 0.89606488434965459, 0.92340042054671068, 0.91799339140883152, 0.89035746470411536, 0.92430159206969065, 0.93271252628416945, 0.94953439471312706, 0.9206969059777711, 0.94412736557524779, 0.95764493841994591, 0.95223790928206664, 0.94322619405226793, 0.94743166115950739, 0.90808050465605283, 0.95974767197356559, 0.97356563532592366, 0.95914689095824568, 0.96515470111144486, 0.97536797837188349, 0.98197656954040258, 0.98468008410934216, 0.99519375187744064, 0.9720636827876239, 0.99489336136978068, 0.9888855512165815, 0.99639531390808056, 0.98648242715530188, 0.99579453289276054, 0.98738359867828174, 0.99519375187744064, 0.99789726644638033, 0.9921898468008411, 0.98918594172424157, 0.99579453289276054, 0.99669570441574051, 0.99759687593872037, 0.99759687593872037, 0.98407930309402225, 0.99939921898468009, 0.99849804746170023, 0.99339140883148092, 0.99429258035446078, 0.99849804746170023, 0.99819765695404028, 0.99579453289276054, 0.9921898468008411, 0.99969960949234005, 0.99849804746170023, 0.99399218984680082, 0.99879843796936019, 1.0, 0.99369179933914087, 0.99759687593872037, 0.99969960949234005, 0.99879843796936019, 0.99939921898468009, 0.98498047461700211, 0.99429258035446078, 0.99849804746170023, 0.99909882847702014, 0.99879843796936019, 0.99969960949234005, 0.99969960949234005, 0.99969960949234005, 0.99909882847702014, 0.99729648543106042, 0.99639531390808056, 0.99789726644638033, 0.99789726644638033, 0.99849804746170023, 0.99969960949234005, 0.99729648543106042, 0.99969960949234005, 0.99939921898468009, 0.99699609492340047, 0.99969960949234005, 0.99969960949234005, 0.99969960949234005, 0.99789726644638033, 0.99939921898468009, 0.99759687593872037, 1.0, 0.99879843796936019, 0.99849804746170023, 0.98978672273956148, 0.99819765695404028, 0.99969960949234005, 0.99939921898468009, 0.99909882847702014, 0.99579453289276054, 0.99939921898468009, 0.99969960949234005, 0.99939921898468009, 0.99789726644638033, 0.99969960949234005, 1.0, 0.99429258035446078, 0.99969960949234005, 0.99939921898468009, 0.99909882847702014, 0.99429258035446078, 0.99849804746170023, 0.99819765695404028, 0.99849804746170023, 0.99939921898468009, 0.99969960949234005, 0.99369179933914087, 0.99729648543106042, 1.0, 0.99939921898468009, 0.99879843796936019, 0.99939921898468009, 0.99849804746170023, 0.99969960949234005, 1.0, 1.0, 0.99969960949234005, 0.99969960949234005, 0.99879843796936019, 0.99639531390808056, 0.99939921898468009, 0.99969960949234005, 0.99849804746170023, 0.99939921898468009, 0.99849804746170023, 0.99489336136978068, 0.99879843796936019, 1.0, 1.0, 0.99939921898468009, 1.0, 0.99939921898468009, 0.99939921898468009, 0.99939921898468009, 0.99879843796936019, 0.99939921898468009, 1.0, 0.99939921898468009, 0.99789726644638033, 0.99969960949234005, 0.99879843796936019, 0.99969960949234005, 1.0, 0.99969960949234005, 0.99939921898468009, 0.99969960949234005, 0.99939921898468009, 0.9960949234004205, 0.99909882847702014, 1.0, 0.99939921898468009, 1.0, 0.99939921898468009, 0.99939921898468009, 0.99909882847702014, 0.99579453289276054, 0.99939921898468009, 0.99909882847702014, 0.99939921898468009, 0.99909882847702014, 1.0, 0.99939921898468009, 0.99969960949234005, 0.99969960949234005, 0.99969960949234005, 0.99909882847702014, 0.99909882847702014, 1.0, 1.0, 0.99909882847702014, 0.99759687593872037, 0.99909882847702014, 0.99909882847702014, 1.0, 1.0, 0.99819765695404028, 0.99969960949234005, 0.99969960949234005, 0.99969960949234005, 1.0, 0.99939921898468009, 0.99939921898468009, 1.0, 0.99939921898468009, 0.99909882847702014, 0.99969960949234005, 0.99969960949234005], 'precision': [0.82246661429693635, 0.81524317912218269, 0.82170313424009467, 0.85902543671467979, 0.91264529764001412, 0.83067274800456103, 0.88933415536374849, 0.81446876668446344, 0.8979397781299524, 0.91350143632301306, 0.87812775978804825, 0.90764925373134331, 0.95185593531789781, 0.88463785046728971, 0.89895366218236172, 0.9284158107687519, 0.89256678281068524, 0.92915779872301607, 0.93354330708661415, 0.92597050857658747, 0.92576028622540252, 0.91836141778036029, 0.96383647798742134, 0.96057457212713937, 0.93407559331966017, 0.9661688509600731, 0.97033374536464767, 0.9514328808446455, 0.97108898168968838, 0.98794063079777361, 0.97239723972397241, 0.98762759047324467, 0.98922413793103448, 0.9884322678843227, 0.97494780793319413, 0.98408886220354252, 0.937995469988675, 0.98448433221782783, 0.99013452914798206, 0.99007518796992477, 0.99251944943147818, 0.98945465501657126, 0.99192100538599637, 0.99365175332527211, 0.99789156626506026, 0.9940155595451825, 0.98157503714710248, 0.99516470232698695, 0.99729241877256314, 0.99579831932773111, 0.99312200956937802, 0.99550359712230219, 0.99665348341953153, 0.99730215827338131, 0.98869720404521122, 0.99758672699849171, 0.99788965933072049, 0.99461400359066432, 0.99550629119233069, 0.99699248120300754, 0.9901079136690647, 0.99640718562874253, 0.99879807692307687, 0.99788902291917969, 0.99461561471731974, 0.98055964653902794, 0.99969779389543667, 0.99819657348963031, 0.9970041941282205, 0.99759975997599759, 0.99879915941158814, 0.99544626593806917, 0.99849170437405732, 0.99194270367054616, 0.99551032624962588, 0.99909855769230771, 0.9970041941282205, 0.99790104947526237, 0.99640718562874253, 0.99909882847702014, 0.99311995213879745, 0.99759398496240603, 0.99879735417919424, 0.99789726644638033, 0.9978985289702792, 0.99372947148402513, 0.99639855942376954, 0.99313637720083559, 0.99610778443113768, 0.98779761904761909, 0.99909936955869105, 0.99909936955869105, 0.99581089168162773, 0.99909774436090226, 0.99640610961365683, 0.99282511210762336, 0.99850029994001199, 0.99879843796936019, 0.99969924812030075, 0.9975779594308205, 0.99849759615384615, 0.99939939939939937, 0.99670461354104256, 0.99939903846153844, 0.99489795918367352, 0.99790041991601675, 0.99879951980792314, 0.99819981998199825, 0.99939831528279177, 0.99879951980792314, 0.99850029994001199, 0.99459134615384615, 0.99879951980792314, 0.99909909909909911, 0.99969942891493835, 0.99819059107358266, 0.99909828674481516, 0.99939849624060151, 0.99939867708959707, 0.99879915941158814, 0.99909936955869105, 0.99668574871949378, 0.99909720132410473, 0.99820089955022484, 0.99819981998199825, 0.99909855769230771, 0.99640610961365683, 0.99819819819819822, 0.99551301226443312, 0.99820089955022484, 0.99790167865707435, 0.99939939939939937, 0.99790104947526237, 0.99849849849849848, 0.99819440264820947, 0.99939921898468009, 0.99939939939939937, 0.99909828674481516, 0.99939921898468009, 0.99909828674481516, 0.99909502262443439, 0.99969933854479853, 0.99820089955022484, 0.99820089955022484, 0.99909909909909911, 0.99820089955022484, 0.99819981998199825, 0.99939921898468009, 0.99909909909909911, 0.99789915966386555, 0.99819981998199825, 0.99670658682634727, 0.99909909909909911, 0.99819711538461542, 0.99879951980792314, 0.99849849849849848, 0.98988697204045206, 0.9987998799879988, 0.99670560047918544, 0.99849939975990398, 0.99909936955869105, 0.99700329637398866, 0.99192342207597961, 0.99969942891493835, 0.9987998799879988, 0.99969951923076927, 0.99820089955022484, 0.99730215827338131, 0.99939921898468009, 0.99939903846153844, 0.99789283564118003, 0.99879915941158814, 0.99969942891493835, 0.99879915941158814, 0.99760047990401923, 0.98812704066488577, 0.9976011994002999, 0.9973029667365898, 0.99909936955869105, 0.99909936955869105, 0.99969942891493835, 0.99939903846153844, 0.99820089955022484, 0.9987998799879988, 0.99939903846153844, 0.99969897652016859, 0.99849894926448512, 0.99819927971188471, 0.99909963985594241, 0.99909963985594241, 0.99461239149955105, 0.99909936955869105, 0.99879951980792314, 0.99640718562874253, 0.99790167865707435, 0.99939921898468009, 0.99969951923076927, 0.99850029994001199, 0.99939921898468009, 0.99969942891493835, 0.99939939939939937, 0.99909936955869105]}}\n"
     ]
    }
   ],
   "source": [
    "print(metrics.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(2)\n",
    "y1 = metrics.metrics['test']['loss']\n",
    "y2 = metrics.metrics['train']['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Learning Curve')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a073d00b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y1, label=\"test\")\n",
    "plt.plot(y2, label=\"train\")\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (CE)')\n",
    "plt.title('Learning Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DL]",
   "language": "python",
   "name": "conda-env-DL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
